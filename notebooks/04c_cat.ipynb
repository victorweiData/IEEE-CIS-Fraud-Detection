{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CatBoost\n",
    "# ============================================================\n",
    "\n",
    "# ---------- General imports ----------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, warnings, random, math, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "import shap\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def seed_everything(seed=43):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def make_dt_m(transaction_dt: pd.Series, days_per_month: int = 30) -> pd.Series:\n",
    "    \"\"\"IEEE-style month index from TransactionDT (seconds since reference).\"\"\"\n",
    "    s = pd.to_numeric(transaction_dt, errors='coerce')\n",
    "    return (s / (3600*24*days_per_month)).fillna(0).astype(np.int16)\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Loading preprocessed features...\n",
      "Train shape: (590540, 791)\n",
      "Test shape: (506691, 791)\n",
      "Features to remove: 19\n",
      "Removing 19 features\n",
      "Final feature count: 772\n",
      "['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339', 'is_december', 'is_holiday', 'card1_fq_enc', 'card2_fq_enc', 'card3_fq_enc', 'card5_fq_enc', 'uid_fq_enc', 'uid2_fq_enc', 'uid3_fq_enc', 'uid4_fq_enc', 'uid5_fq_enc', 'card3_DT_D_hour_dist', 'card3_DT_W_week_day_dist', 'card3_DT_M_month_day_dist', 'card3_DT_D_hour_dist_best', 'card3_DT_W_week_day_dist_best', 'card3_DT_M_month_day_dist_best', 'card5_DT_D_hour_dist', 'card5_DT_W_week_day_dist', 'card5_DT_M_month_day_dist', 'card5_DT_D_hour_dist_best', 'card5_DT_W_week_day_dist_best', 'card5_DT_M_month_day_dist_best', 'bank_type_DT_D_hour_dist', 'bank_type_DT_W_week_day_dist', 'bank_type_DT_M_month_day_dist', 'bank_type_DT_D_hour_dist_best', 'bank_type_DT_W_week_day_dist_best', 'bank_type_DT_M_month_day_dist_best', 'bank_type_DT_M', 'bank_type_DT_W', 'bank_type_DT_D', 'uid_D1_mean', 'uid_D1_std', 'uid2_D1_mean', 'uid2_D1_std', 'uid3_D1_mean', 'uid3_D1_std', 'uid4_D1_mean', 'uid4_D1_std', 'uid5_D1_mean', 'uid5_D1_std', 'bank_type_D1_mean', 'bank_type_D1_std', 'uid_D2_mean', 'uid_D2_std', 'uid2_D2_mean', 'uid2_D2_std', 'uid3_D2_mean', 'uid3_D2_std', 'uid4_D2_mean', 'uid4_D2_std', 'uid5_D2_mean', 'uid5_D2_std', 'bank_type_D2_mean', 'bank_type_D2_std', 'uid_D3_mean', 'uid_D3_std', 'uid2_D3_mean', 'uid2_D3_std', 'uid3_D3_mean', 'uid3_D3_std', 'uid4_D3_mean', 'uid4_D3_std', 'uid5_D3_mean', 'uid5_D3_std', 'bank_type_D3_mean', 'bank_type_D3_std', 'uid_D4_mean', 'uid_D4_std', 'uid2_D4_mean', 'uid2_D4_std', 'uid3_D4_mean', 'uid3_D4_std', 'uid4_D4_mean', 'uid4_D4_std', 'uid5_D4_mean', 'uid5_D4_std', 'bank_type_D4_mean', 'bank_type_D4_std', 'uid_D5_mean', 'uid_D5_std', 'uid2_D5_mean', 'uid2_D5_std', 'uid3_D5_mean', 'uid3_D5_std', 'uid4_D5_mean', 'uid4_D5_std', 'uid5_D5_mean', 'uid5_D5_std', 'bank_type_D5_mean', 'bank_type_D5_std', 'uid_D6_mean', 'uid_D6_std', 'uid2_D6_mean', 'uid2_D6_std', 'uid3_D6_mean', 'uid3_D6_std', 'uid4_D6_mean', 'uid4_D6_std', 'uid5_D6_mean', 'uid5_D6_std', 'bank_type_D6_mean', 'bank_type_D6_std', 'uid_D7_mean', 'uid_D7_std', 'uid2_D7_mean', 'uid2_D7_std', 'uid3_D7_mean', 'uid3_D7_std', 'uid4_D7_mean', 'uid4_D7_std', 'uid5_D7_mean', 'uid5_D7_std', 'bank_type_D7_mean', 'bank_type_D7_std', 'uid_D8_mean', 'uid_D8_std', 'uid2_D8_mean', 'uid2_D8_std', 'uid3_D8_mean', 'uid3_D8_std', 'uid4_D8_mean', 'uid4_D8_std', 'uid5_D8_mean', 'uid5_D8_std', 'bank_type_D8_mean', 'bank_type_D8_std', 'uid_D9_mean', 'uid_D9_std', 'uid2_D9_mean', 'uid2_D9_std', 'uid3_D9_mean', 'uid3_D9_std', 'uid4_D9_mean', 'uid4_D9_std', 'uid5_D9_mean', 'uid5_D9_std', 'bank_type_D9_mean', 'bank_type_D9_std', 'uid_D10_mean', 'uid_D10_std', 'uid2_D10_mean', 'uid2_D10_std', 'uid3_D10_mean', 'uid3_D10_std', 'uid4_D10_mean', 'uid4_D10_std', 'uid5_D10_mean', 'uid5_D10_std', 'bank_type_D10_mean', 'bank_type_D10_std', 'uid_D11_mean', 'uid_D11_std', 'uid2_D11_mean', 'uid2_D11_std', 'uid3_D11_mean', 'uid3_D11_std', 'uid4_D11_mean', 'uid4_D11_std', 'uid5_D11_mean', 'uid5_D11_std', 'bank_type_D11_mean', 'bank_type_D11_std', 'uid_D12_mean', 'uid_D12_std', 'uid2_D12_mean', 'uid2_D12_std', 'uid3_D12_mean', 'uid3_D12_std', 'uid4_D12_mean', 'uid4_D12_std', 'uid5_D12_mean', 'uid5_D12_std', 'bank_type_D12_mean', 'bank_type_D12_std', 'uid_D13_mean', 'uid_D13_std', 'uid2_D13_mean', 'uid2_D13_std', 'uid3_D13_mean', 'uid3_D13_std', 'uid4_D13_mean', 'uid4_D13_std', 'uid5_D13_mean', 'uid5_D13_std', 'bank_type_D13_mean', 'bank_type_D13_std', 'uid_D14_mean', 'uid_D14_std', 'uid2_D14_mean', 'uid2_D14_std', 'uid3_D14_mean', 'uid3_D14_std', 'uid4_D14_mean', 'uid4_D14_std', 'uid5_D14_mean', 'uid5_D14_std', 'bank_type_D14_mean', 'bank_type_D14_std', 'uid_D15_mean', 'uid_D15_std', 'uid2_D15_mean', 'uid2_D15_std', 'uid3_D15_mean', 'uid3_D15_std', 'uid4_D15_mean', 'uid4_D15_std', 'uid5_D15_mean', 'uid5_D15_std', 'bank_type_D15_mean', 'bank_type_D15_std', 'D9_not_na', 'D8_not_same_day', 'D8_D9_decimal_dist', 'D3_DT_D_min_max', 'D3_DT_D_std_score', 'D4_DT_D_min_max', 'D4_DT_D_std_score', 'D5_DT_D_min_max', 'D5_DT_D_std_score', 'D6_DT_D_min_max', 'D6_DT_D_std_score', 'D7_DT_D_min_max', 'D7_DT_D_std_score', 'D8_DT_D_min_max', 'D8_DT_D_std_score', 'D10_DT_D_min_max', 'D10_DT_D_std_score', 'D11_DT_D_min_max', 'D11_DT_D_std_score', 'D12_DT_D_min_max', 'D12_DT_D_std_score', 'D13_DT_D_min_max', 'D13_DT_D_std_score', 'D14_DT_D_min_max', 'D14_DT_D_std_score', 'D15_DT_D_min_max', 'D15_DT_D_std_score', 'D3_DT_W_min_max', 'D3_DT_W_std_score', 'D4_DT_W_min_max', 'D4_DT_W_std_score', 'D5_DT_W_min_max', 'D5_DT_W_std_score', 'D6_DT_W_min_max', 'D6_DT_W_std_score', 'D7_DT_W_min_max', 'D7_DT_W_std_score', 'D8_DT_W_min_max', 'D8_DT_W_std_score', 'D10_DT_W_min_max', 'D10_DT_W_std_score', 'D11_DT_W_min_max', 'D11_DT_W_std_score', 'D12_DT_W_min_max', 'D12_DT_W_std_score', 'D13_DT_W_min_max', 'D13_DT_W_std_score', 'D14_DT_W_min_max', 'D14_DT_W_std_score', 'D15_DT_W_min_max', 'D15_DT_W_std_score', 'D3_DT_M_min_max', 'D3_DT_M_std_score', 'D4_DT_M_min_max', 'D4_DT_M_std_score', 'D5_DT_M_min_max', 'D5_DT_M_std_score', 'D6_DT_M_min_max', 'D6_DT_M_std_score', 'D7_DT_M_min_max', 'D7_DT_M_std_score', 'D8_DT_M_min_max', 'D8_DT_M_std_score', 'D10_DT_M_min_max', 'D10_DT_M_std_score', 'D11_DT_M_min_max', 'D11_DT_M_std_score', 'D12_DT_M_min_max', 'D12_DT_M_std_score', 'D13_DT_M_min_max', 'D13_DT_M_std_score', 'D14_DT_M_min_max', 'D14_DT_M_std_score', 'D15_DT_M_min_max', 'D15_DT_M_std_score', 'D1_scaled', 'D2_scaled', 'TransactionAmt_check', 'card1_TransactionAmt_mean', 'card1_TransactionAmt_std', 'card2_TransactionAmt_mean', 'card2_TransactionAmt_std', 'card3_TransactionAmt_mean', 'card3_TransactionAmt_std', 'card5_TransactionAmt_mean', 'card5_TransactionAmt_std', 'uid_TransactionAmt_mean', 'uid_TransactionAmt_std', 'uid2_TransactionAmt_mean', 'uid2_TransactionAmt_std', 'uid3_TransactionAmt_mean', 'uid3_TransactionAmt_std', 'uid4_TransactionAmt_mean', 'uid4_TransactionAmt_std', 'uid5_TransactionAmt_mean', 'uid5_TransactionAmt_std', 'bank_type_TransactionAmt_mean', 'bank_type_TransactionAmt_std', 'TransactionAmt_DT_D_min_max', 'TransactionAmt_DT_D_std_score', 'TransactionAmt_DT_W_min_max', 'TransactionAmt_DT_W_std_score', 'TransactionAmt_DT_M_min_max', 'TransactionAmt_DT_M_std_score', 'product_type', 'product_type_DT_D', 'product_type_DT_W', 'product_type_DT_M', 'C1_fq_enc', 'C2_fq_enc', 'C3_fq_enc', 'C4_fq_enc', 'C5_fq_enc', 'C6_fq_enc', 'C7_fq_enc', 'C8_fq_enc', 'C9_fq_enc', 'C10_fq_enc', 'C11_fq_enc', 'C12_fq_enc', 'C13_fq_enc', 'C14_fq_enc', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'id_33_0', 'id_33_1', 'DeviceInfo_device', 'DeviceInfo_version', 'id_30_device', 'id_30_version', 'id_31_device']\n"
     ]
    }
   ],
   "source": [
    "# ---------- DATA LOAD (YOUR APPROACH) ----------\n",
    "print('Loading preprocessed data...')\n",
    "DATA_DIR = \"../data/processed\"\n",
    "\n",
    "# Load preprocessed features from kyakovlev's kernel\n",
    "print(\"Loading preprocessed features...\")\n",
    "train = pd.read_pickle(f\"{DATA_DIR}/train_df.pkl\")\n",
    "test = pd.read_pickle(f\"{DATA_DIR}/test_df.pkl\")\n",
    "remove_features_df = pd.read_pickle(f\"{DATA_DIR}/remove_features.pkl\")\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Features to remove: {len(remove_features_df)}\")\n",
    "\n",
    "# Get features to remove\n",
    "remove_features = list(remove_features_df['features_to_remove'].values)\n",
    "print(f\"Removing {len(remove_features)} features\")\n",
    "\n",
    "# Build final feature list (exclude removed features)\n",
    "all_features = [col for col in train.columns if col not in remove_features]\n",
    "\n",
    "# Exclude target and ID columns\n",
    "EXCLUDE_COLS = {'TransactionID', 'isFraud', 'DT_M'}\n",
    "FEATURES = [col for col in all_features if col not in EXCLUDE_COLS]\n",
    "\n",
    "print(f\"Final feature count: {len(FEATURES)}\")\n",
    "print(FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing feature types...\n",
      "  ProductCD: float64, 5 unique -> categorical\n",
      "  card1: 9117 unique (too many) -> numeric\n",
      "  card2: float16, 496 unique -> numeric (too many categories)\n",
      "  card3: float16, 96 unique -> categorical\n",
      "  card4: float32, 4 unique -> categorical\n",
      "  card5: float16, 83 unique -> categorical\n",
      "  card6: float32, 3 unique -> categorical\n",
      "  addr1: float16, 332 unique -> numeric (too many categories)\n",
      "  addr2: float16, 74 unique -> categorical\n",
      "  P_emaildomain: object/category -> categorical\n",
      "  R_emaildomain: object/category -> categorical\n",
      "  M1: float16, 2 unique -> categorical\n",
      "  M2: float16, 2 unique -> categorical\n",
      "  M3: float16, 2 unique -> categorical\n",
      "  M4: float64, 3 unique -> categorical\n",
      "  M5: float16, 2 unique -> categorical\n",
      "  M6: float16, 2 unique -> categorical\n",
      "  M7: float16, 2 unique -> categorical\n",
      "  M8: float16, 2 unique -> categorical\n",
      "  M9: float16, 2 unique -> categorical\n",
      "  id_12: float64, 2 unique -> categorical\n",
      "  id_13: float16, 54 unique -> categorical\n",
      "  id_14: float16, 25 unique -> categorical\n",
      "  id_15: float16, 3 unique -> categorical\n",
      "  id_16: float16, 2 unique -> categorical\n",
      "  id_17: float16, 104 unique -> categorical\n",
      "  id_18: float16, 18 unique -> categorical\n",
      "  id_19: float16, 522 unique -> numeric (too many categories)\n",
      "  id_20: float16, 394 unique -> numeric (too many categories)\n",
      "  id_21: float16, 490 unique -> numeric (too many categories)\n",
      "  id_22: float16, 25 unique -> categorical\n",
      "  id_23: float16, 2 unique -> categorical\n",
      "  id_24: float16, 12 unique -> categorical\n",
      "  id_25: float16, 341 unique -> numeric (too many categories)\n",
      "  id_26: float16, 95 unique -> categorical\n",
      "  id_27: float16, 2 unique -> categorical\n",
      "  id_28: float16, 2 unique -> categorical\n",
      "  id_29: float16, 2 unique -> categorical\n",
      "  id_30: int64, 72 unique -> categorical\n",
      "  id_31: int64, 108 unique -> categorical\n",
      "  id_32: float16, 4 unique -> categorical\n",
      "  id_33: int64, 105 unique -> categorical\n",
      "  id_34: float16, 3 unique -> categorical\n",
      "  id_35: float16, 2 unique -> categorical\n",
      "  id_36: float16, 2 unique -> categorical\n",
      "  id_37: float16, 2 unique -> categorical\n",
      "  id_38: float16, 2 unique -> categorical\n",
      "  DeviceType: object/category -> categorical\n",
      "  DeviceInfo: int64, 190 unique -> categorical\n",
      "\n",
      "Identified 42 categorical features\n",
      "Categorical features:\n",
      "  - ProductCD (float64, 5 unique)\n",
      "  - card3 (float16, 96 unique)\n",
      "  - card4 (float32, 4 unique)\n",
      "  - card5 (float16, 83 unique)\n",
      "  - card6 (float32, 3 unique)\n",
      "  - addr2 (float16, 74 unique)\n",
      "  - P_emaildomain (category, 60 unique)\n",
      "  - R_emaildomain (category, 61 unique)\n",
      "  - M1 (float16, 2 unique)\n",
      "  - M2 (float16, 2 unique)\n",
      "  - M3 (float16, 2 unique)\n",
      "  - M4 (float64, 3 unique)\n",
      "  - M5 (float16, 2 unique)\n",
      "  - M6 (float16, 2 unique)\n",
      "  - M7 (float16, 2 unique)\n",
      "  - M8 (float16, 2 unique)\n",
      "  - M9 (float16, 2 unique)\n",
      "  - id_12 (float64, 2 unique)\n",
      "  - id_13 (float16, 54 unique)\n",
      "  - id_14 (float16, 25 unique)\n",
      "  - id_15 (float16, 3 unique)\n",
      "  - id_16 (float16, 2 unique)\n",
      "  - id_17 (float16, 104 unique)\n",
      "  - id_18 (float16, 18 unique)\n",
      "  - id_22 (float16, 25 unique)\n",
      "  - id_23 (float16, 2 unique)\n",
      "  - id_24 (float16, 12 unique)\n",
      "  - id_26 (float16, 95 unique)\n",
      "  - id_27 (float16, 2 unique)\n",
      "  - id_28 (float16, 2 unique)\n",
      "  - id_29 (float16, 2 unique)\n",
      "  - id_30 (int64, 72 unique)\n",
      "  - id_31 (int64, 108 unique)\n",
      "  - id_32 (float16, 4 unique)\n",
      "  - id_33 (int64, 105 unique)\n",
      "  - id_34 (float16, 3 unique)\n",
      "  - id_35 (float16, 2 unique)\n",
      "  - id_36 (float16, 2 unique)\n",
      "  - id_37 (float16, 2 unique)\n",
      "  - id_38 (float16, 2 unique)\n",
      "  - DeviceType (category, 3 unique)\n",
      "  - DeviceInfo (int64, 190 unique)\n"
     ]
    }
   ],
   "source": [
    "# ---------- Prepare target and features ----------\n",
    "TARGET = 'isFraud'\n",
    "y = train[TARGET].astype('int8')\n",
    "\n",
    "# Create DT_M for GroupKFold if not already present\n",
    "if 'DT_M' not in train.columns and 'TransactionDT' in train.columns:\n",
    "    print(\"Creating DT_M from TransactionDT...\")\n",
    "    train['DT_M'] = make_dt_m(train['TransactionDT'])\n",
    "elif 'DT_M' not in train.columns:\n",
    "    print(\"Warning: Neither DT_M nor TransactionDT found. Using row-based groups.\")\n",
    "    # Fallback: create pseudo-groups based on row order\n",
    "    train['DT_M'] = (np.arange(len(train)) // (len(train) // 6)).astype('int16')\n",
    "\n",
    "# ---------- Identify categorical features ----------\n",
    "# For preprocessed data, we need to identify categorical features properly\n",
    "print(\"Analyzing feature types...\")\n",
    "\n",
    "# Start with an empty list\n",
    "CAT_FEATURES = []\n",
    "\n",
    "# Check each potential categorical feature\n",
    "potential_cat_features = [\n",
    "    'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "    'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain',\n",
    "    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n",
    "    'DeviceType', 'DeviceInfo'\n",
    "] + [f'id_{i}' for i in range(12, 39)]\n",
    "\n",
    "for col in FEATURES:\n",
    "    if col in train.columns:\n",
    "        col_dtype = train[col].dtype\n",
    "        unique_count = train[col].nunique()\n",
    "        \n",
    "        # Check if it's a string/object column (definitely categorical)\n",
    "        if col_dtype == 'object' or col_dtype.name == 'category':\n",
    "            CAT_FEATURES.append(col)\n",
    "            print(f\"  {col}: object/category -> categorical\")\n",
    "            \n",
    "        # Check if it's in our known categorical list AND has reasonable cardinality\n",
    "        elif col in potential_cat_features:\n",
    "            # Additional checks for preprocessed data\n",
    "            if unique_count <= 1000:  # Reasonable limit for categorical\n",
    "                # Check if values look categorical\n",
    "                sample_values = train[col].dropna().head(1000)\n",
    "                if len(sample_values) > 0:\n",
    "                    # If few unique values, likely categorical regardless of dtype\n",
    "                    if unique_count <= 200:  # Most categorical features have < 200 categories\n",
    "                        CAT_FEATURES.append(col)\n",
    "                        print(f\"  {col}: {col_dtype}, {unique_count} unique -> categorical\")\n",
    "                    else:\n",
    "                        print(f\"  {col}: {col_dtype}, {unique_count} unique -> numeric (too many categories)\")\n",
    "                else:\n",
    "                    print(f\"  {col}: all null -> skipping\")\n",
    "            else:\n",
    "                print(f\"  {col}: {unique_count} unique (too many) -> numeric\")\n",
    "\n",
    "print(f'\\nIdentified {len(CAT_FEATURES)} categorical features')\n",
    "if CAT_FEATURES:\n",
    "    print(\"Categorical features:\")\n",
    "    for cat in CAT_FEATURES:\n",
    "        print(f\"  - {cat} ({train[cat].dtype}, {train[cat].nunique()} unique)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Handling missing values and categorical conversion...\n",
      "Missing values per feature (top 10):\n",
      "id_23    588860\n",
      "id_24    585793\n",
      "id_25    585408\n",
      "id_08    585385\n",
      "id_07    585385\n",
      "id_21    585381\n",
      "id_26    585377\n",
      "id_22    585371\n",
      "id_27    585371\n",
      "dist2    552913\n",
      "dtype: int64\n",
      "\n",
      "Processing missing values...\n",
      "Numerical features: keeping NaN for CatBoost to handle (730 features)\n",
      "\n",
      "Handling categorical features (42 features)...\n",
      "  Processing ProductCD (float64, 5 unique)\n",
      "    Converting float to string\n",
      "  Processing card3 (float16, 96 unique)\n",
      "    Converting float to string\n",
      "  Processing card4 (float32, 4 unique)\n",
      "    Converting float to string\n",
      "  Processing card5 (float16, 83 unique)\n",
      "    Converting float to string\n",
      "  Processing card6 (float32, 3 unique)\n",
      "    Converting float to string\n",
      "  Processing addr2 (float16, 74 unique)\n",
      "    Converting float to string\n",
      "  Processing P_emaildomain (category, 60 unique)\n",
      "    Converting pandas Categorical to string\n",
      "  Processing R_emaildomain (category, 61 unique)\n",
      "    Converting pandas Categorical to string\n",
      "  Processing M1 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing M2 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing M3 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing M4 (float64, 3 unique)\n",
      "    Converting float to string\n",
      "  Processing M5 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing M6 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing M7 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing M8 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing M9 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing id_12 (float64, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing id_13 (float16, 54 unique)\n",
      "    Converting float to string\n",
      "  Processing id_14 (float16, 25 unique)\n",
      "    Converting float to string\n",
      "  Processing id_15 (float16, 3 unique)\n",
      "    Converting float to string\n",
      "  Processing id_16 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing id_17 (float16, 104 unique)\n",
      "    Converting float to string\n",
      "  Processing id_18 (float16, 18 unique)\n",
      "    Converting float to string\n",
      "  Processing id_22 (float16, 25 unique)\n",
      "    Converting float to string\n",
      "  Processing id_23 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing id_24 (float16, 12 unique)\n",
      "    Converting float to string\n",
      "  Processing id_26 (float16, 95 unique)\n",
      "    Converting float to string\n",
      "  Processing id_27 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing id_28 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing id_29 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing id_30 (int64, 72 unique)\n",
      "    Converting int to string\n",
      "  Processing id_31 (int64, 108 unique)\n",
      "    Converting int to string\n",
      "  Processing id_32 (float16, 4 unique)\n",
      "    Converting float to string\n",
      "  Processing id_33 (int64, 105 unique)\n",
      "    Converting int to string\n",
      "  Processing id_34 (float16, 3 unique)\n",
      "    Converting float to string\n",
      "  Processing id_35 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing id_36 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing id_37 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing id_38 (float16, 2 unique)\n",
      "    Converting float to string\n",
      "  Processing DeviceType (category, 3 unique)\n",
      "    Converting pandas Categorical to string\n",
      "  Processing DeviceInfo (int64, 190 unique)\n",
      "    Converting int to string\n",
      "Categorical features converted successfully!\n",
      "Sample categorical values:\n",
      "  ProductCD: ['0']\n",
      "  card3: ['150' '117' '185' '143' '144']\n",
      "  card4: ['9524' '347386' '719649' '16009' 'missing']\n"
     ]
    }
   ],
   "source": [
    "# ---------- Handle missing values and convert categorical features ----------\n",
    "print(\"\\nHandling missing values and categorical conversion...\")\n",
    "\n",
    "# Handle missing values BEFORE converting categorical features\n",
    "print(f\"Missing values per feature (top 10):\")\n",
    "missing_counts = train[FEATURES].isnull().sum().sort_values(ascending=False)\n",
    "print(missing_counts.head(10))\n",
    "\n",
    "# Handle categorical and numerical features separately for missing values\n",
    "print(f\"\\nProcessing missing values...\")\n",
    "\n",
    "# For numerical features, we can use NaN (CatBoost handles this automatically)\n",
    "numerical_features = [f for f in FEATURES if f not in CAT_FEATURES]\n",
    "print(f\"Numerical features: keeping NaN for CatBoost to handle ({len(numerical_features)} features)\")\n",
    "\n",
    "# For categorical features, we need to handle missing values more carefully\n",
    "print(f\"\\nHandling categorical features ({len(CAT_FEATURES)} features)...\")\n",
    "for cat_col in CAT_FEATURES:\n",
    "    if cat_col in train.columns:\n",
    "        original_dtype = train[cat_col].dtype\n",
    "        print(f\"  Processing {cat_col} ({original_dtype}, {train[cat_col].nunique()} unique)\")\n",
    "        \n",
    "        # Handle categorical columns (they might be pandas Categorical type)\n",
    "        if original_dtype.name == 'category':\n",
    "            # For pandas Categorical, convert to string first, then handle missing\n",
    "            print(f\"    Converting pandas Categorical to string\")\n",
    "            train_col = train[cat_col].astype(str)\n",
    "            test_col = test[cat_col].astype(str)\n",
    "            # Replace 'nan' strings with 'missing'\n",
    "            train_col = train_col.replace('nan', 'missing')\n",
    "            test_col = test_col.replace('nan', 'missing')\n",
    "            train[cat_col] = train_col\n",
    "            test[cat_col] = test_col\n",
    "            \n",
    "        elif 'float' in str(original_dtype):\n",
    "            print(f\"    Converting float to string\")\n",
    "            # Fill NaN first, then convert\n",
    "            train_col = train[cat_col].fillna(-999)\n",
    "            test_col = test[cat_col].fillna(-999)\n",
    "            # Convert to int then string\n",
    "            train[cat_col] = train_col.round().astype(int).astype(str)\n",
    "            test[cat_col] = test_col.round().astype(int).astype(str)\n",
    "            # Replace the fill value with a cleaner missing indicator\n",
    "            train[cat_col] = train[cat_col].replace('-999', 'missing')\n",
    "            test[cat_col] = test[cat_col].replace('-999', 'missing')\n",
    "            \n",
    "        elif 'int' in str(original_dtype):\n",
    "            print(f\"    Converting int to string\")\n",
    "            # Fill NaN first, then convert\n",
    "            train_col = train[cat_col].fillna(-999)\n",
    "            test_col = test[cat_col].fillna(-999)\n",
    "            train[cat_col] = train_col.astype(int).astype(str)\n",
    "            test[cat_col] = test_col.astype(int).astype(str)\n",
    "            # Replace the fill value with a cleaner missing indicator\n",
    "            train[cat_col] = train[cat_col].replace('-999', 'missing')\n",
    "            test[cat_col] = test[cat_col].replace('-999', 'missing')\n",
    "            \n",
    "        else:  # object type\n",
    "            print(f\"    Converting object to string\")\n",
    "            train[cat_col] = train[cat_col].fillna('missing').astype(str)\n",
    "            test[cat_col] = test[cat_col].fillna('missing').astype(str)\n",
    "\n",
    "print(\"Categorical features converted successfully!\")\n",
    "print(f\"Sample categorical values:\")\n",
    "for cat_col in CAT_FEATURES[:3]:  # Show first 3 as examples\n",
    "    unique_vals = train[cat_col].unique()[:5]  # First 5 unique values\n",
    "    print(f\"  {cat_col}: {unique_vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall fraud rate: 0.0350 (3.50%) | scale_pos_weight: 27.58\n",
      "\n",
      "============================================================\n",
      "TRAINING CATBOOST WITH GROUPKFOLD (DT_M)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Folds:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 | holdout month = 12 | train=453219 valid=137321\n",
      "Fold 0 AUC: 0.9128 | Best iter: 1932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Folds:  17%|█▋        | 1/6 [55:32<4:37:43, 3332.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 | holdout month = 15 | train=488908 valid=101632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Folds:  33%|███▎      | 2/6 [1:25:57<2:43:02, 2445.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 AUC: 0.9342 | Best iter: 1401\n",
      "\n",
      "Fold 2 | holdout month = 13 | train=497955 valid=92585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Folds:  50%|█████     | 3/6 [1:57:23<1:49:31, 2190.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 AUC: 0.9335 | Best iter: 1446\n",
      "\n",
      "Fold 3 | holdout month = 17 | train=501214 valid=89326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Folds:  67%|██████▋   | 4/6 [2:22:35<1:04:05, 1922.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 AUC: 0.9294 | Best iter: 1127\n",
      "\n",
      "Fold 4 | holdout month = 14 | train=504519 valid=86021\n",
      "Fold 4 AUC: 0.9368 | Best iter: 1710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Folds:  83%|████████▎ | 5/6 [2:59:31<33:48, 2028.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 5 | holdout month = 16 | train=506885 valid=83655\n",
      "Fold 5 AUC: 0.9455 | Best iter: 1919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Folds: 100%|██████████| 6/6 [3:40:27<00:00, 2204.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################\n",
      "CatBoost OOF CV = 0.9311\n",
      "Fold AUCs: [0.9128 0.9342 0.9335 0.9294 0.9368 0.9455]\n",
      "Weighted mean AUC: 0.9304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== GroupKFold by month ==========\n",
    "assert 'DT_M' in train.columns, \"DT_M not found. Check data preprocessing.\"\n",
    "groups = train['DT_M'].values\n",
    "skf = GroupKFold(n_splits=6)\n",
    "folds = list(skf.split(train[FEATURES], y, groups=groups))\n",
    "\n",
    "# Class imbalance\n",
    "pos = int(y.sum())\n",
    "neg = int(len(y) - pos)\n",
    "scale_pos_weight = (neg / max(pos, 1))\n",
    "print(f\"\\nOverall fraud rate: {y.mean():.4f} ({y.mean()*100:.2f}%) | scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# CatBoost parameters\n",
    "cb_params = {\n",
    "    \"loss_function\": \"Logloss\",\n",
    "    \"eval_metric\": \"AUC\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"depth\": 8,\n",
    "    \"l2_leaf_reg\": 3.0,\n",
    "    \"iterations\": 5000,  # Reasonable cap with early stopping\n",
    "    \"random_seed\": SEED,\n",
    "    \"thread_count\": -1,\n",
    "    \"logging_level\": \"Silent\",\n",
    "    \"task_type\": \"CPU\",  # Change to 'GPU' if available\n",
    "    \"use_best_model\": True,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "}\n",
    "\n",
    "# Create directories for artifacts\n",
    "ARTIFACT_DIR = \"catboost_artifacts\"\n",
    "SUB_DIR = \"submissions\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "os.makedirs(SUB_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING CATBOOST WITH GROUPKFOLD (DT_M)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = []\n",
    "oof = np.zeros(len(train), dtype=float)\n",
    "fold_scores, fold_sizes = [], []\n",
    "\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(tqdm(folds, desc=\"Folds\")):\n",
    "    va_month = int(groups[va_idx][0])\n",
    "    print(f\"\\nFold {fold_id} | holdout month = {va_month} | train={len(tr_idx)} valid={len(va_idx)}\")\n",
    "    \n",
    "    X_tr = train.iloc[tr_idx][FEATURES]\n",
    "    y_tr = y.iloc[tr_idx]\n",
    "    X_va = train.iloc[va_idx][FEATURES]\n",
    "    y_va = y.iloc[va_idx]\n",
    "    \n",
    "    # Create CatBoost pools\n",
    "    dtr = Pool(X_tr, label=y_tr, cat_features=CAT_FEATURES)\n",
    "    dva = Pool(X_va, label=y_va, cat_features=CAT_FEATURES)\n",
    "    \n",
    "    # Train model\n",
    "    model = CatBoostClassifier(**cb_params)\n",
    "    model.fit(\n",
    "        dtr,\n",
    "        eval_set=dva,\n",
    "        early_stopping_rounds=200,\n",
    "        verbose=200\n",
    "    )\n",
    "    \n",
    "    # Predict on validation set\n",
    "    va_pred = model.predict_proba(dva)[:,1]\n",
    "    oof[va_idx] = va_pred\n",
    "    auc = roc_auc_score(y_va, va_pred)\n",
    "    fold_scores.append(auc)\n",
    "    fold_sizes.append(len(va_idx))\n",
    "    print(f\"Fold {fold_id} AUC: {auc:.4f} | Best iter: {model.get_best_iteration()}\")\n",
    "    \n",
    "    # Save model\n",
    "    model.save_model(f\"{ARTIFACT_DIR}/catboost_fold{fold_id}.cbm\")\n",
    "    models.append(model)\n",
    "    \n",
    "    # Clean up memory\n",
    "    del X_tr, y_tr, X_va, y_va, dtr, dva, va_pred\n",
    "    gc.collect()\n",
    "\n",
    "# Calculate final scores\n",
    "fold_scores = np.array(fold_scores, dtype=float)\n",
    "weights = np.array(fold_sizes, dtype=float) / np.sum(fold_sizes)\n",
    "weighted_cv = float(np.sum(fold_scores * weights))\n",
    "oof_auc = roc_auc_score(y, oof)\n",
    "\n",
    "print(\"\\n\" + \"#\"*20)\n",
    "print(f\"CatBoost OOF CV = {oof_auc:.4f}\")\n",
    "print(\"Fold AUCs:\", np.round(fold_scores, 4))\n",
    "print(\"Weighted mean AUC:\", f\"{weighted_cv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating feature importance...\n",
      "Top 10 most important features:\n",
      "           feature  importance\n",
      "711      C1_fq_enc    2.112462\n",
      "47              M5    1.668027\n",
      "26             C13    1.478920\n",
      "27             C14    1.256796\n",
      "14              C1    1.227671\n",
      "12   P_emaildomain    0.993845\n",
      "48              M6    0.976447\n",
      "723     C13_fq_enc    0.916317\n",
      "679      D2_scaled    0.798277\n",
      "2            card1    0.765258\n",
      "\n",
      "==============================\n",
      "GENERATING SUBMISSION\n",
      "==============================\n",
      "Fold 0 test preds | mean=0.10694 std=0.18043\n",
      "Fold 1 test preds | mean=0.13298 std=0.19028\n",
      "Fold 2 test preds | mean=0.13045 std=0.19023\n",
      "Fold 3 test preds | mean=0.14955 std=0.19738\n",
      "Fold 4 test preds | mean=0.11930 std=0.18564\n",
      "Fold 5 test preds | mean=0.11293 std=0.18078\n",
      "Saved submission: submissions/submission_catboost_preprocessed_cv0.9311.csv\n",
      "\n",
      "============================================================\n",
      "CATBOOST GROUPKFOLD COMPLETE\n",
      "============================================================\n",
      "Final OOF AUC: 0.9311\n",
      "Number of features used: 772\n",
      "Number of categorical features: 42\n",
      "Features removed by preprocessing: 19\n"
     ]
    }
   ],
   "source": [
    "# ---------- Save artifacts ----------\n",
    "np.save(f\"{ARTIFACT_DIR}/catboost_oof_predictions.npy\", oof)\n",
    "\n",
    "metadata = {\n",
    "    \"model_type\": \"catboost_groupkfold_preprocessed\",\n",
    "    \"oof_auc\": oof_auc,\n",
    "    \"weighted_cv_auc\": weighted_cv,\n",
    "    \"fold_scores\": fold_scores.tolist(),\n",
    "    \"fold_weights\": weights.tolist(),\n",
    "    \"n_folds\": len(folds),\n",
    "    \"params\": cb_params,\n",
    "    \"features\": FEATURES,\n",
    "    \"categorical_features\": CAT_FEATURES,\n",
    "    \"validation_method\": \"GroupKFold with DT_M\",\n",
    "    \"best_iterations\": [int(m.get_best_iteration()) for m in models],\n",
    "    \"data_source\": \"kyakovlev_preprocessed\",\n",
    "    \"features_removed\": len(remove_features)\n",
    "}\n",
    "\n",
    "with open(f\"{ARTIFACT_DIR}/catboost_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "# Feature importance (averaged across folds)\n",
    "print(\"\\nCalculating feature importance...\")\n",
    "importances = []\n",
    "for (tr_idx, _), m in zip(folds, models):\n",
    "    tr_pool = Pool(train.iloc[tr_idx][FEATURES], label=y.iloc[tr_idx], cat_features=CAT_FEATURES)\n",
    "    imp = m.get_feature_importance(tr_pool, type=\"PredictionValuesChange\")\n",
    "    importances.append(np.array(imp, dtype=float))\n",
    "    del tr_pool; gc.collect()\n",
    "\n",
    "fi_values = np.mean(np.vstack(importances), axis=0)\n",
    "fi_df = pd.DataFrame({\"feature\": FEATURES, \"importance\": fi_values}).sort_values(\"importance\", ascending=False)\n",
    "fi_df.to_csv(f\"{ARTIFACT_DIR}/catboost_feature_importance.csv\", index=False)\n",
    "\n",
    "print(\"Top 10 most important features:\")\n",
    "print(fi_df.head(10))\n",
    "\n",
    "# ---------- Test predictions + submission ----------\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"GENERATING SUBMISSION\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Create test pool\n",
    "DTEST = Pool(test[FEATURES], cat_features=CAT_FEATURES)\n",
    "\n",
    "# Generate predictions\n",
    "test_preds = np.zeros(len(test), dtype=float)\n",
    "for i, m in enumerate(models):\n",
    "    p = m.predict_proba(DTEST)[:,1]\n",
    "    test_preds += p / len(models)\n",
    "    print(f\"Fold {i} test preds | mean={p.mean():.5f} std={p.std():.5f}\")\n",
    "\n",
    "np.save(f\"{ARTIFACT_DIR}/catboost_test_predictions.npy\", test_preds)\n",
    "\n",
    "# Create submission file\n",
    "# Check if we have TransactionID in test data\n",
    "if 'TransactionID' in test.columns:\n",
    "    submission = pd.DataFrame({\n",
    "        'TransactionID': test['TransactionID'],\n",
    "        'isFraud': test_preds\n",
    "    })\n",
    "else:\n",
    "    # If no TransactionID, create a basic submission\n",
    "    submission = pd.DataFrame({\n",
    "        'TransactionID': range(len(test_preds)),\n",
    "        'isFraud': test_preds\n",
    "    })\n",
    "\n",
    "sub_path = f\"{SUB_DIR}/submission_catboost_preprocessed_cv{oof_auc:.4f}.csv\"\n",
    "submission.to_csv(sub_path, index=False)\n",
    "print(f\"Saved submission: {sub_path}\")\n",
    "\n",
    "# Final summary\n",
    "summary = {\n",
    "    \"model_name\": \"catboost_groupkfold_preprocessed\",\n",
    "    \"performance\": {\n",
    "        \"oof_auc\": oof_auc,\n",
    "        \"weighted_cv_auc\": weighted_cv,\n",
    "        \"fold_aucs\": fold_scores.tolist()\n",
    "    },\n",
    "    \"data_info\": {\n",
    "        \"n_features\": len(FEATURES),\n",
    "        \"n_categorical\": len(CAT_FEATURES),\n",
    "        \"features_removed\": len(remove_features),\n",
    "        \"data_source\": \"kyakovlev_preprocessed\"\n",
    "    },\n",
    "    \"files_saved\": [\n",
    "        \"catboost_oof_predictions.npy\",\n",
    "        \"catboost_test_predictions.npy\",\n",
    "        \"catboost_metadata.json\",\n",
    "        \"catboost_feature_importance.csv\"\n",
    "    ] + [f\"catboost_fold{i}.cbm\" for i in range(len(models))]\n",
    "}\n",
    "\n",
    "with open(f\"{ARTIFACT_DIR}/catboost_ensemble_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CATBOOST GROUPKFOLD COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final OOF AUC: {oof_auc:.4f}\")\n",
    "print(f\"Number of features used: {len(FEATURES)}\")\n",
    "print(f\"Number of categorical features: {len(CAT_FEATURES)}\")\n",
    "print(f\"Features removed by preprocessing: {len(remove_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieee-cis-fraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
