{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2235b8ee",
   "metadata": {},
   "source": [
    "# IEEE Fraud Detection - Complete Feature Engineering Pipeline\n",
    "\n",
    "This notebook implements a comprehensive fraud detection pipeline with:\n",
    "- Data loading and preprocessing\n",
    "- V-column correlation analysis and reduction\n",
    "- Feature engineering with UID aggregates\n",
    "- Walk-forward time validation (6 months)\n",
    "- Time-consistent target encoding\n",
    "- Feature stability testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c081d",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08fd1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loading test data...\n",
      "Train shape: (590540, 434)\n",
      "Test shape: (506691, 433)\n",
      "Train columns: 434\n",
      "Test columns: 433\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Globals & Constants ---\n",
    "PATH = \"../data/raw/\"\n",
    "TRAIN_TRAN = PATH + \"train_transaction.csv\"\n",
    "TRAIN_ID = PATH + \"train_identity.csv\"\n",
    "TEST_TRAN = PATH + \"test_transaction.csv\"\n",
    "TEST_ID = PATH + \"test_identity.csv\"\n",
    "TARGET = \"isfraud\"\n",
    "\n",
    "# Load CSVs\n",
    "print(\"Loading training data...\")\n",
    "train_transaction = pd.read_csv(TRAIN_TRAN)\n",
    "train_identity = pd.read_csv(TRAIN_ID)\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_transaction = pd.read_csv(TEST_TRAN)\n",
    "test_identity = pd.read_csv(TEST_ID)\n",
    "\n",
    "# Merge train\n",
    "train_df = train_transaction.merge(train_identity, on=\"TransactionID\", how=\"left\")\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "\n",
    "# Merge test\n",
    "test_df = test_transaction.merge(test_identity, on=\"TransactionID\", how=\"left\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# Quick peek\n",
    "print(\"Train columns:\", train_df.shape[1])\n",
    "print(\"Test columns:\", test_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12219a9a",
   "metadata": {},
   "source": [
    "## 2. Column Normalization and High-Missing Column Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbbacdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 9 columns (>98% missing in both train/test)\n",
      "Initial clean up - Train: 425 Test: 424\n"
     ]
    }
   ],
   "source": [
    "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = df.columns.str.lower().str.replace(\"-\", \"_\").str.replace(\" \", \"_\")\n",
    "    return df\n",
    "\n",
    "def drop_high_missing(train_df: pd.DataFrame, test_df: pd.DataFrame, thresh: float = 0.98):\n",
    "    miss_train = train_df.isna().mean()\n",
    "    miss_test = test_df.isna().mean()\n",
    "    drop_cols = [c for c in train_df.columns\n",
    "                 if (miss_train.get(c,0) > thresh) and (miss_test.get(c,0) > thresh)]\n",
    "    keep_cols = [c for c in train_df.columns if c not in drop_cols]\n",
    "    print(f\"Dropping {len(drop_cols)} columns (>{thresh*100:.0f}% missing in both train/test)\")\n",
    "    return (train_df[keep_cols],\n",
    "            test_df[[c for c in keep_cols if c in test_df.columns]],\n",
    "            drop_cols)\n",
    "\n",
    "train_df = normalize_cols(train_df)\n",
    "test_df = normalize_cols(test_df)\n",
    "train_df, test_df, dropped_cols = drop_high_missing(train_df, test_df, thresh=0.98)\n",
    "print(\"Initial clean up - Train:\", len(train_df.columns), \"Test:\", len(test_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6c080",
   "metadata": {},
   "source": [
    "## 3. V-Column Correlation Analysis and Reduction\n",
    "\n",
    "Based on Kaggle EDA analysis, we reduce correlated V columns from groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542f6d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V-columns: keeping 128, dropping 211\n",
      "V-columns: keeping 128, dropping 211\n",
      "After V-reduction - Train: (590540, 214), Test: (506691, 213)\n"
     ]
    }
   ],
   "source": [
    "# V-column reduction based on correlation analysis\n",
    "# These are the optimal V columns identified from correlation analysis\n",
    "V_REDUCED = [\n",
    "    # V1-V11 block: [1, 3, 4, 6, 8, 11]\n",
    "    1, 3, 4, 6, 8, 11,\n",
    "    # V12-V34 block: [13, 14, 17, 20, 23, 26, 27, 30] \n",
    "    13, 14, 17, 20, 23, 26, 27, 30,\n",
    "    # V35-V52 block: [36, 37, 40, 41, 44, 47, 48]\n",
    "    36, 37, 40, 41, 44, 47, 48,\n",
    "    # V53-V74 block: [54, 56, 59, 62, 65, 67, 68, 70]\n",
    "    54, 56, 59, 62, 65, 67, 68, 70,\n",
    "    # V75-V94 block: [76, 78, 80, 82, 86, 88, 89, 91]\n",
    "    76, 78, 80, 82, 86, 88, 89, 91,\n",
    "    # V95-V106 block: [96, 98, 99, 104]\n",
    "    96, 98, 99, 104,\n",
    "    # V107-V123 block: [107, 108, 111, 115, 117, 120, 121, 123]\n",
    "    107, 108, 111, 115, 117, 120, 121, 123,\n",
    "    # V124-V137 block: [124, 127, 129, 130, 136]\n",
    "    124, 127, 129, 130, 136,\n",
    "    # V138-V163 blocks\n",
    "    138, 139, 142, 147, 156, 162, 165, 160, 166,\n",
    "    # V167-V216 blocks  \n",
    "    178, 176, 173, 182, 187, 203, 205, 207, 215,\n",
    "    169, 171, 175, 180, 185, 188, 198, 210, 209,\n",
    "    # V217-V278 blocks\n",
    "    218, 223, 224, 226, 228, 229, 235,\n",
    "    240, 258, 257, 253, 252, 260, 261,\n",
    "    264, 266, 267, 274, 277,\n",
    "    220, 221, 234, 238, 250, 271,\n",
    "    # V279-V321 blocks\n",
    "    294, 284, 285, 286, 291, 297,\n",
    "    303, 305, 307, 309, 310, 320,\n",
    "    # V281-V315 block: [281, 283, 289, 296, 301, 314]\n",
    "    281, 283, 289, 296, 301, 314,\n",
    "    # V322-V339 block: [332, 325, 335, 338]\n",
    "    332, 325, 335, 338\n",
    "]\n",
    "\n",
    "def apply_v_reduction(df: pd.DataFrame, v_keep: list) -> pd.DataFrame:\n",
    "    v_cols_all = [c for c in df.columns if c.startswith('v') and c[1:].isdigit()]\n",
    "    v_cols_keep = [f'v{i}' for i in v_keep if f'v{i}' in df.columns]\n",
    "    v_cols_drop = [c for c in v_cols_all if c not in v_cols_keep]\n",
    "    print(f\"V-columns: keeping {len(v_cols_keep)}, dropping {len(v_cols_drop)}\")\n",
    "    return df.drop(columns=v_cols_drop)\n",
    "\n",
    "train_df = apply_v_reduction(train_df, V_REDUCED)\n",
    "test_df = apply_v_reduction(test_df, V_REDUCED)\n",
    "print(f\"After V-reduction - Train: {train_df.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfaaa3e",
   "metadata": {},
   "source": [
    "## 4. Base Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94ad1c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base feature engineering...\n",
      "Base engineering complete - Train: (590540, 227), Test: (506691, 226)\n"
     ]
    }
   ],
   "source": [
    "def as_str_key(s: pd.Series, na_token=\"__na__\") -> pd.Series:\n",
    "    return s.astype(\"string\").fillna(na_token).astype(str)\n",
    "\n",
    "DOMAIN_MAP = {\n",
    "    \"googlemail.com\": \"gmail.com\", \"hotmail.co.uk\": \"hotmail.com\", \"hotmail.fr\": \"hotmail.com\",\n",
    "    \"ymail.com\": \"yahoo.com\", \"yahoo.co.jp\": \"yahoo.jp\", \"yahoo.co.uk\": \"yahoo.com\",\n",
    "    \"live.com.mx\": \"live.com\", \"outlook.com.br\": \"outlook.com\", \"icloud.com.cn\": \"icloud.com\",\n",
    "}\n",
    "MULTIPART_TLDS = {\"co.uk\",\"ac.uk\",\"gov.uk\",\"com.au\",\"net.au\",\"org.au\",\"com.br\",\"com.ar\",\n",
    "                  \"com.mx\",\"com.tr\",\"com.cn\",\"com.hk\",\"com.sg\",\"co.jp\"}\n",
    "FREEMAIL = {\"gmail.com\",\"yahoo.com\",\"yahoo.jp\",\"hotmail.com\",\"outlook.com\",\"live.com\",\n",
    "            \"aol.com\",\"icloud.com\",\"me.com\",\"mac.com\",\"msn.com\",\"protonmail.com\",\n",
    "            \"gmx.com\",\"gmx.de\",\"yandex.ru\",\"mail.ru\",\"qq.com\",\"163.com\",\n",
    "            \"126.com\",\"sina.com\",\"sohu.com\",\"orange.fr\",\"free.fr\",\"wanadoo.fr\",\n",
    "            \"libero.it\",\"web.de\",\"naver.com\"}\n",
    "\n",
    "def normalize_domain(x: str) -> str:\n",
    "    if not isinstance(x, str): return \"__na__\"\n",
    "    d = x.strip().lower()\n",
    "    return DOMAIN_MAP.get(d, d)\n",
    "\n",
    "def parent_domain(d: str) -> str:\n",
    "    if d in (\"__na__\", \"\", None): return \"__na__\"\n",
    "    parts = d.split(\".\")\n",
    "    if len(parts) < 2: return d\n",
    "    last2 = \".\".join(parts[-2:])\n",
    "    last3 = \".\".join(parts[-3:])\n",
    "    return last3 if last2 in MULTIPART_TLDS and len(parts) >= 3 else last2\n",
    "\n",
    "def add_email_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for side in [\"p\", \"r\"]:\n",
    "        src_col = f\"{side}_emaildomain\"\n",
    "        s = df[src_col].apply(normalize_domain) if src_col in df.columns else pd.Series([\"__na__\"]*len(df))\n",
    "        par = s.apply(parent_domain)\n",
    "        df[f\"{side}_ed_parent\"] = par\n",
    "        df[f\"{side}_is_freemail\"] = par.isin(FREEMAIL).astype(\"int8\")\n",
    "    df[\"email_parent_match\"] = (df[\"p_ed_parent\"] == df[\"r_ed_parent\"]).astype(\"int8\")\n",
    "    return df\n",
    "\n",
    "def add_device_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def os_family(x):\n",
    "        if not isinstance(x, str): return \"__na__\"\n",
    "        x = x.lower()\n",
    "        if \"windows\" in x: return \"windows\"\n",
    "        if \"ios\" in x: return \"ios\"\n",
    "        if \"mac\" in x: return \"macos\"\n",
    "        if \"android\" in x: return \"android\"\n",
    "        if \"linux\" in x: return \"linux\"\n",
    "        return \"__other__\"\n",
    "    df[\"id_30_os\"] = df[\"id_30\"].apply(os_family) if \"id_30\" in df else pd.Series([\"__na__\"]*len(df))\n",
    "    return df\n",
    "\n",
    "def add_time_primitives(df: pd.DataFrame, col=\"transactiondt\") -> pd.DataFrame:\n",
    "    if col not in df.columns: raise KeyError(f\"Expected '{col}' column\")\n",
    "    dt = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype(np.int64)\n",
    "    df[\"dt\"] = dt\n",
    "    df[\"day\"] = (dt // (24 * 60 * 60)).astype(np.int32)\n",
    "    df[\"hour\"] = ((dt // (60 * 60)) % 24).astype(np.int16)\n",
    "    df[\"dow\"] = (df[\"day\"] % 7).astype(np.int8)\n",
    "    return df\n",
    "\n",
    "def add_amount_primitives(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    amt = pd.to_numeric(df[\"transactionamt\"], errors=\"coerce\") if \"transactionamt\" in df else pd.Series([np.nan]*len(df))\n",
    "    df[\"log_amt\"] = np.log1p(amt.astype(float))\n",
    "    df[\"amt_cents\"] = np.round((amt.astype(float) % 1) * 100).astype(\"float32\")\n",
    "    df[\"amt_is_round\"] = ((amt.astype(float) % 1) == 0).astype(\"int8\")\n",
    "    return df\n",
    "\n",
    "print(\"Base feature engineering...\")\n",
    "train_df = add_email_features(train_df)\n",
    "train_df = add_device_features(train_df)\n",
    "train_df = add_time_primitives(train_df, col=\"transactiondt\")\n",
    "train_df = add_amount_primitives(train_df)\n",
    "\n",
    "test_df = add_email_features(test_df)\n",
    "test_df = add_device_features(test_df)\n",
    "test_df = add_time_primitives(test_df, col=\"transactiondt\")\n",
    "test_df = add_amount_primitives(test_df)\n",
    "print(f\"Base engineering complete - Train: {train_df.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7b97a",
   "metadata": {},
   "source": [
    "## 5. UID Creation and Magic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6553db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating leak-safe aggregates for 'uid2'...\n",
      "Generating leak-safe aggregates for 'uid3'...\n",
      "Performing leak-safe frequency encoding for 'productcd'...\n",
      "Performing leak-safe frequency encoding for 'card1'...\n",
      "Performing leak-safe frequency encoding for 'card2'...\n",
      "Performing leak-safe frequency encoding for 'card3'...\n",
      "Performing leak-safe frequency encoding for 'card4'...\n",
      "Performing leak-safe frequency encoding for 'card5'...\n",
      "Performing leak-safe frequency encoding for 'card6'...\n",
      "Performing leak-safe frequency encoding for 'addr1'...\n",
      "Performing leak-safe frequency encoding for 'addr2'...\n",
      "Performing leak-safe frequency encoding for 'p_ed_parent'...\n",
      "Performing leak-safe frequency encoding for 'r_ed_parent'...\n",
      "Performing leak-safe frequency encoding for 'id_30_os'...\n",
      "Performing leak-safe frequency encoding for 'devicetype'...\n",
      "Performing leak-safe frequency encoding for 'uid1'...\n",
      "Performing leak-safe frequency encoding for 'uid2'...\n",
      "Performing leak-safe frequency encoding for 'uid3'...\n",
      "Performing leak-safe frequency encoding for 'uid4'...\n",
      "\n",
      "Final shapes after leak-safe engineering:\n",
      "Train: (590540, 276), Test: (506691, 275)\n",
      "Target stats: isfraud\n",
      "0    0.965\n",
      "1    0.035\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def add_uids(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    c1 = as_str_key(df.get(\"card1\"))\n",
    "    a1 = as_str_key(df.get(\"addr1\"))\n",
    "    ped = as_str_key(df.get(\"p_ed_parent\"))\n",
    "    df[\"uid1\"] = c1\n",
    "    df[\"uid2\"] = c1 + \"_\" + a1\n",
    "    df[\"uid3\"] = df[\"uid2\"] + \"_\" + ped\n",
    "    if \"d1\" in df.columns and \"day\" in df.columns:\n",
    "        anch = np.floor((df[\"day\"].astype(\"float64\") - pd.to_numeric(df[\"d1\"], errors=\"coerce\"))).astype(\"Int64\")\n",
    "        anch = anch.astype(\"string\").fillna(\"__na__\").astype(str)\n",
    "    else:\n",
    "        anch = pd.Series([\"__na__\"] * len(df), index=df.index)\n",
    "    df[\"uid4\"] = anch + \"_\" + ped\n",
    "    return df\n",
    "\n",
    "# Create UIDs on train and test separately\n",
    "train_df = add_uids(train_df)\n",
    "test_df = add_uids(test_df)\n",
    "\n",
    "# --- CORRECTED: LEAK-SAFE AGGREGATE FEATURE ENGINEERING ---\n",
    "# Define a single function that fits on a training set and transforms both\n",
    "# training and test sets. This ensures no information from the test set\n",
    "# leaks into the model.\n",
    "\n",
    "def add_agg_features(train: pd.DataFrame, test: pd.DataFrame):\n",
    "    all_dfs = {\"train\": train, \"test\": test}\n",
    "    \n",
    "    # 5.1 UID Time Aggregates (Leak-safe)\n",
    "    for k in [\"uid2\", \"uid3\"]:\n",
    "        print(f\"Generating leak-safe aggregates for '{k}'...\")\n",
    "        \n",
    "        # Fit on training data ONLY\n",
    "        agg_train = train.groupby(k).agg(\n",
    "            mean_dt=( \"dt\", lambda x: x.diff().mean()),\n",
    "            std_dt=( \"dt\", lambda x: x.diff().std()),\n",
    "            mean_amt=(\"transactionamt\", \"mean\"),\n",
    "            std_amt=(\"transactionamt\", \"std\"),\n",
    "            count_amt=(\"transactionamt\", \"count\"),\n",
    "        ).add_suffix(\"_\" + k).astype(\"float32\")\n",
    "        \n",
    "        # Merge with both train and test\n",
    "        all_dfs[\"train\"] = all_dfs[\"train\"].merge(agg_train, on=k, how=\"left\")\n",
    "        all_dfs[\"test\"] = all_dfs[\"test\"].merge(agg_train, on=k, how=\"left\")\n",
    "        \n",
    "        # Add past-only deltas (must be done on sorted data)\n",
    "        train = train.sort_values([k, \"dt\"])\n",
    "        test = test.sort_values([k, \"dt\"])\n",
    "        train[f\"{k}_secs_since_prev\"] = train.groupby(k)[\"dt\"].diff().astype(\"float32\")\n",
    "        test[f\"{k}_secs_since_prev\"] = test.groupby(k)[\"dt\"].diff().astype(\"float32\")\n",
    "        \n",
    "        all_dfs[\"train\"][f\"{k}_secs_since_prev\"] = train[f\"{k}_secs_since_prev\"]\n",
    "        all_dfs[\"test\"][f\"{k}_secs_since_prev\"] = test[f\"{k}_secs_since_prev\"]\n",
    "        \n",
    "        # Add a log transformation of the delta\n",
    "        all_dfs[\"train\"][f\"{k}_secs_since_prev_log\"] = np.log1p(\n",
    "            all_dfs[\"train\"][f\"{k}_secs_since_prev\"].fillna(0).clip(0, 3*24*3600)\n",
    "        ).astype(\"float32\")\n",
    "        all_dfs[\"test\"][f\"{k}_secs_since_prev_log\"] = np.log1p(\n",
    "            all_dfs[\"test\"][f\"{k}_secs_since_prev\"].fillna(0).clip(0, 3*24*3600)\n",
    "        ).astype(\"float32\")\n",
    "        \n",
    "        # Add absolute deviation\n",
    "        all_dfs[\"train\"][f\"{k}_amt_devabs\"] = (\n",
    "            all_dfs[\"train\"][\"transactionamt\"] - all_dfs[\"train\"][f\"mean_amt_{k}\"]\n",
    "        ).abs().astype(\"float32\")\n",
    "        all_dfs[\"test\"][f\"{k}_amt_devabs\"] = (\n",
    "            all_dfs[\"test\"][\"transactionamt\"] - all_dfs[\"test\"][f\"mean_amt_{k}\"]\n",
    "        ).abs().astype(\"float32\")\n",
    "        \n",
    "    # 5.2 Frequency Encoding (Leak-safe)\n",
    "    freq_cols = [\n",
    "        \"productcd\",\"card1\",\"card2\",\"card3\",\"card4\",\"card5\",\"card6\",\n",
    "        \"addr1\",\"addr2\",\"p_ed_parent\",\"r_ed_parent\",\n",
    "        \"id_30_os\",\"id_31_br\",\"devicetype\",\"devicebrand\",\"res_bucket\",\n",
    "        \"uid1\",\"uid2\",\"uid3\",\"uid4\"\n",
    "    ]\n",
    "    for col in freq_cols:\n",
    "        if col not in train.columns: continue\n",
    "        print(f\"Performing leak-safe frequency encoding for '{col}'...\")\n",
    "        vc = train[col].value_counts(dropna=False)\n",
    "        all_dfs[\"train\"][col + \"_fe\"] = all_dfs[\"train\"][col].map(vc).astype(\"float32\")\n",
    "        all_dfs[\"test\"][col + \"_fe\"] = all_dfs[\"test\"][col].map(vc).astype(\"float32\")\n",
    "\n",
    "    # 5.3 Missingness Signatures (Leak-safe)\n",
    "    def add_missingness_signatures(df: pd.DataFrame):\n",
    "        miss = df.isna().sum(axis=1).astype(\"int32\")\n",
    "        df[\"na_ct_all\"] = miss\n",
    "        df[\"na_ratio_all\"] = (miss / df.shape[1]).astype(\"float32\")\n",
    "        for fam in [\"c\",\"d\",\"m\",\"v\",\"id_\"]:\n",
    "            fam_cols = [c for c in df.columns if c.startswith(fam)]\n",
    "            if not fam_cols: continue\n",
    "            mct = df[fam_cols].isna().sum(axis=1).astype(\"int32\")\n",
    "            df[f\"na_ct_{fam}\"] = mct\n",
    "            df[f\"na_ratio_{fam}\"] = (mct / max(1, len(fam_cols))).astype(\"float32\")\n",
    "        return df\n",
    "\n",
    "    all_dfs[\"train\"] = add_missingness_signatures(all_dfs[\"train\"])\n",
    "    all_dfs[\"test\"] = add_missingness_signatures(all_dfs[\"test\"])\n",
    "    \n",
    "    return all_dfs[\"train\"], all_dfs[\"test\"]\n",
    "\n",
    "# Apply the leak-safe feature engineering\n",
    "train_df, test_df = add_agg_features(train_df, test_df)\n",
    "\n",
    "print(\"\\nFinal shapes after leak-safe engineering:\")\n",
    "print(f\"Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# Clean up target\n",
    "if TARGET in test_df.columns:\n",
    "    test_df.drop(columns=[TARGET], inplace=True)\n",
    "if TARGET in train_df.columns:\n",
    "    train_df[TARGET] = pd.to_numeric(train_df[TARGET], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
    "print(\"Target stats:\", train_df[TARGET].value_counts(normalize=True).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae08a4",
   "metadata": {},
   "source": [
    "## 6. Expanding Walk-Forward Time Folds (6 Months)\n",
    "\n",
    "Modified to use 6 months of data with proper expanding walk-forward validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7175215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train months [0, 1, 2] -> Valid months [3] (Train: 315927, Valid: 98615)\n",
      "Fold 2: Train months [0, 1, 2, 3] -> Valid months [4] (Train: 414542, Valid: 83571)\n",
      "Fold 3: Train months [0, 1, 2, 3, 4] -> Valid months [5] (Train: 498113, Valid: 86934)\n",
      "Fold 4: Train months [0, 1, 2, 3, 4, 5] -> Valid months [6] (Train: 585047, Valid: 5493)\n",
      "\n",
      "Created 4 walk-forward folds\n"
     ]
    }
   ],
   "source": [
    "def add_month_index(df: pd.DataFrame, days_per_month: int = 30, col_day: str = \"day\") -> pd.DataFrame:\n",
    "    if col_day not in df.columns: raise KeyError(\"Expected 'day' column. Run time primitives first.\")\n",
    "    d0 = int(df[col_day].min())\n",
    "    df[\"month_ix\"] = ((df[col_day] - d0) // days_per_month).astype(\"int16\")\n",
    "    return df\n",
    "\n",
    "def make_expanding_folds(df, group_col=\"month_ix\", min_train_months=3, valid_months=1, min_train_rows=50000, min_valid_rows=10000):\n",
    "    groups = np.sort(df[group_col].unique())\n",
    "    folds = []\n",
    "    for end in range(min_train_months, len(groups) - valid_months + 1):\n",
    "        tr_groups = set(groups[:end])\n",
    "        va_groups = set(groups[end:end+valid_months])\n",
    "        tr_idx = df.index[df[group_col].isin(tr_groups)].to_numpy()\n",
    "        va_idx = df.index[df[group_col].isin(va_groups)].to_numpy()\n",
    "        if len(tr_idx) >= min_train_rows and len(va_idx) >= min_valid_rows:\n",
    "            folds.append((tr_idx, va_idx))\n",
    "            print(f\"Fold {len(folds)}: Train months {list(tr_groups)} -> Valid months {list(va_groups)} \"\n",
    "                  f\"(Train: {len(tr_idx)}, Valid: {len(va_idx)})\")\n",
    "    return folds\n",
    "\n",
    "train_df = add_month_index(train_df, days_per_month=30, col_day=\"day\")\n",
    "folds = make_expanding_folds(train_df, group_col=\"month_ix\", min_train_months=3, valid_months=1, min_train_rows=50000, min_valid_rows=1000)\n",
    "print(f\"\\nCreated {len(folds)} walk-forward folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f40803",
   "metadata": {},
   "source": [
    "## 7. Time-Aligned Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48082326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk-forward OOF-TE done for 17 cols | OOF coverage: 0.465\n",
      "Encoded columns: ['uid1_te_wf', 'uid2_te_wf', 'uid3_te_wf', 'uid4_te_wf', 'productcd_te_wf']\n"
     ]
    }
   ],
   "source": [
    "def _fit_target_mean(train_y, train_cat, m=100.0, prior=None):\n",
    "    y = pd.Series(train_y, index=train_cat.index)\n",
    "    if prior is None: prior = float(y.mean())\n",
    "    stats = pd.DataFrame({\"y\": y.values, \"c\": train_cat.values})\n",
    "    grp = stats.groupby(\"c\")[\"y\"].agg([\"sum\",\"count\"])\n",
    "    enc = (grp[\"sum\"] + prior * m) / (grp[\"count\"] + m)\n",
    "    return enc.astype(\"float32\"), float(prior)\n",
    "\n",
    "def _apply_target_mean(cat_series, enc_map, prior):\n",
    "    vals = cat_series.map(enc_map)\n",
    "    return vals.fillna(prior).astype(\"float32\")\n",
    "\n",
    "def oof_target_encode_walkforward(train_df, test_df, cols, folds, target=TARGET, m_smooth=100.0, suffix=\"_te_wf\"):\n",
    "    if target not in train_df.columns: raise KeyError(f\"Target '{target}' not found\")\n",
    "    y_all = pd.to_numeric(train_df[target], errors=\"coerce\").fillna(0).astype(\"int8\")\n",
    "    oof_mat = {f\"{c}{suffix}\": np.full(len(train_df), np.nan, dtype=\"float32\") for c in cols}\n",
    "    test_mat = {f\"{c}{suffix}\": None for c in cols}\n",
    "    for c in cols:\n",
    "        if c not in train_df.columns:\n",
    "            print(f\"[OOF-TE] skip '{c}' (missing)\")\n",
    "            continue\n",
    "        tr_cat = train_df[c].astype(\"string\").fillna(\"__na__\")\n",
    "        te_cat = test_df[c].astype(\"string\").fillna(\"__na__\") if c in test_df.columns else pd.Series([\"__na__\"]*len(test_df))\n",
    "        for k, (tr_idx, va_idx) in enumerate(folds, 1):\n",
    "            enc_map, prior = _fit_target_mean(y_all.iloc[tr_idx], tr_cat.iloc[tr_idx], m=m_smooth)\n",
    "            oof_vals = _apply_target_mean(tr_cat.iloc[va_idx], enc_map, prior)\n",
    "            oof_mat[f\"{c}{suffix}\"][va_idx] = oof_vals.values\n",
    "        enc_full, prior_full = _fit_target_mean(y_all, tr_cat, m=m_smooth)\n",
    "        test_vals = _apply_target_mean(te_cat, enc_full, prior_full)\n",
    "        test_mat[f\"{c}{suffix}\"] = test_vals.values.astype(\"float32\")\n",
    "    for new_col, arr in oof_mat.items(): train_df[new_col] = arr\n",
    "    for new_col, arr in test_mat.items():\n",
    "        if arr is not None: test_df[new_col] = arr\n",
    "    first = next(iter(oof_mat.values()))\n",
    "    cov = float(np.mean(~np.isnan(first)))\n",
    "    print(f\"Walk-forward OOF-TE done for {len(cols)} cols | OOF coverage: {cov:.3f}\")\n",
    "    return train_df, test_df\n",
    "\n",
    "te_cols = [\n",
    "    \"uid1\",\"uid2\",\"uid3\",\"uid4\", \"productcd\",\"card1\",\"card2\",\"card3\",\"card4\",\"card5\",\"card6\",\n",
    "    \"addr1\",\"addr2\", \"p_ed_parent\",\"r_ed_parent\", \"id_30_os\",\"id_31_br\",\"devicetype\",\"devicebrand\",\"res_bucket\"\n",
    "]\n",
    "te_cols = [c for c in te_cols if c in train_df.columns]\n",
    "train_df, test_df = oof_target_encode_walkforward(\n",
    "    train_df=train_df, test_df=test_df, cols=te_cols, folds=folds, target=TARGET, m_smooth=100.0, suffix=\"_te_wf\"\n",
    ")\n",
    "print(\"Encoded columns:\", [col for col in train_df.columns if col.endswith(\"_te_wf\")][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c49a5",
   "metadata": {},
   "source": [
    "## 8. Time Consistency Feature Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "993438e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan candidates: 186 (D/M/V/id_ families)\n",
      "Time consistency scan using 4 walk-forward folds\n",
      "[50/186] m5 | val_mean=0.519 gap=0.013\n",
      "[100/186] v203 | val_mean=0.562 gap=0.002\n",
      "[150/186] v309 | val_mean=0.525 gap=0.004\n",
      "Results saved:\n",
      "  - Stable features: 222\n",
      "  - Unstable features: 8\n",
      "  - Final keep list: 222\n",
      "\n",
      "Top 10 stable features by validation AUC:\n",
      "  feature  val_auc_mean  gap_train_minus_val  unstable_flag\n",
      "0   id_35      0.690405            -0.022471          False\n",
      "1   id_15      0.689073            -0.026622          False\n",
      "2   id_29      0.688135            -0.026070          False\n",
      "3   id_28      0.687972            -0.026654          False\n",
      "4   id_38      0.686433            -0.029267          False\n",
      "5      m6      0.686408            -0.030440          False\n",
      "6   id_12      0.685670            -0.035047          False\n",
      "7    v303      0.683048            -0.027043          False\n",
      "8   id_31      0.682733            -0.007584          False\n",
      "9   id_37      0.681379            -0.028526          False\n"
     ]
    }
   ],
   "source": [
    "# Feature stability testing using single-feature models\n",
    "WHITELIST = {\n",
    "    \"transactionamt\",\"log_amt\",\"amt_cents\",\"amt_is_round\",\n",
    "    \"hour\",\"dow\",\n",
    "    \"productcd\",\"card1\",\"card2\",\"card3\",\"card4\",\"card5\",\"card6\", \n",
    "    \"addr1\",\"addr2\",\"p_emaildomain\",\"r_emaildomain\",\"p_ed_parent\",\"r_ed_parent\",\n",
    "    \"devicetype\",\"devicebrand\",\"id_30_os\",\"id_31_br\",\"res_bucket\",\n",
    "    \"dist1\",\"dist2\",\"c1\",\"c2\",\"c3\",\"c4\",\"c5\",\"c6\",\"c7\",\"c8\",\"c9\",\"c10\",\"c11\",\"c12\",\"c13\",\"c14\",\n",
    "    \"email_parent_match\",\n",
    "    \"uid2_secs_since_prev\",\"uid2_secs_since_prev_log\",\"uid2_dt_mean\",\"uid2_dt_std\",\n",
    "    \"uid2_amt_mean\",\"uid2_amt_std\",\"uid2_amt_count\",\"uid2_amt_devabs\",\n",
    "    \"uid3_secs_since_prev\",\"uid3_secs_since_prev_log\",\"uid3_dt_mean\",\"uid3_dt_std\", \n",
    "    \"uid3_amt_mean\",\"uid3_amt_std\",\"uid3_amt_count\",\"uid3_amt_devabs\",\n",
    "}\n",
    "\n",
    "BLACKLIST_EXACT = {TARGET, \"transactionid\", \"transactiondt\", \"dt\", \"month_ix\", \"day\", \"index\"}\n",
    "EXCLUDE_SUFFIXES = (\"_fe\", \"_te_wf\")\n",
    "EXCLUDE_PREFIXES = (\"uid\",)\n",
    "\n",
    "def build_scan_candidates(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Build list of ambiguous features to scan (D/M/V/id_ families)\"\"\"\n",
    "    def is_scan_family(c: str) -> bool:\n",
    "        c = c.lower()\n",
    "        return c.startswith(\"d\") or c.startswith(\"m\") or c.startswith(\"v\") or c.startswith(\"id_\")\n",
    "\n",
    "    out = []\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if cl in BLACKLIST_EXACT or cl in WHITELIST:\n",
    "            continue\n",
    "        if cl.endswith(EXCLUDE_SUFFIXES):\n",
    "            continue\n",
    "        if any(cl.startswith(p) for p in EXCLUDE_PREFIXES):\n",
    "            continue\n",
    "        if is_scan_family(cl):\n",
    "            out.append(c)\n",
    "    \n",
    "    out = sorted(out)\n",
    "    print(f\"Scan candidates: {len(out)} (D/M/V/id_ families)\")\n",
    "    return out\n",
    "\n",
    "def fit_target_mean_single(train_y: pd.Series, train_cat: pd.Series, m: float = 100.0):\n",
    "    \"\"\"Target encoding for single feature\"\"\"\n",
    "    y = pd.Series(pd.to_numeric(train_y, errors=\"coerce\").fillna(0).astype(\"int8\"))\n",
    "    if len(y) == 0:\n",
    "        return pd.Series(dtype=\"float32\"), 0.0\n",
    "    prior = float(y.mean())\n",
    "    stats = pd.DataFrame({\n",
    "        \"y\": y.values, \n",
    "        \"c\": train_cat.astype(\"string\").fillna(\"__na__\").values\n",
    "    })\n",
    "    grp = stats.groupby(\"c\")[\"y\"].agg([\"sum\",\"count\"])\n",
    "    enc = (grp[\"sum\"] + prior * m) / (grp[\"count\"] + m)\n",
    "    return enc.astype(\"float32\"), prior\n",
    "\n",
    "def apply_target_mean_single(cat_series: pd.Series, enc_map: pd.Series, prior: float):\n",
    "    \"\"\"Apply target encoding for single feature\"\"\"\n",
    "    return cat_series.astype(\"string\").fillna(\"__na__\").map(enc_map).fillna(prior).astype(\"float32\")\n",
    "\n",
    "def _finite_median(a: pd.Series):\n",
    "    \"\"\"Get finite median, handling inf/nan\"\"\"\n",
    "    a = pd.to_numeric(a, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    vals = a[np.isfinite(a)]\n",
    "    return float(np.median(vals)) if vals.size > 0 else None\n",
    "\n",
    "def fit_eval_single_feature(train_df: pd.DataFrame, col: str, tr_idx, va_idx, m_smooth=100.0):\n",
    "    \"\"\"Fit and evaluate single feature model\"\"\"\n",
    "    y = train_df.loc[:, TARGET].astype(\"int8\").values\n",
    "    s = train_df[col]\n",
    "    is_num = pd.api.types.is_numeric_dtype(s)\n",
    "\n",
    "    X_tr_raw = s.iloc[tr_idx].copy()\n",
    "    X_va_raw = s.iloc[va_idx].copy()\n",
    "\n",
    "    if is_num:\n",
    "        X_tr_raw = pd.to_numeric(X_tr_raw, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "        X_va_raw = pd.to_numeric(X_va_raw, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "        med = _finite_median(X_tr_raw)\n",
    "        if med is None:\n",
    "            return np.nan, np.nan\n",
    "        X_tr = X_tr_raw.fillna(med).astype(\"float32\").values.reshape(-1,1)\n",
    "        X_va = X_va_raw.fillna(med).astype(\"float32\").values.reshape(-1,1)\n",
    "    else:\n",
    "        enc_map, prior = fit_target_mean_single(\n",
    "            train_df.loc[tr_idx, TARGET],\n",
    "            X_tr_raw.astype(\"string\").fillna(\"__na__\"),\n",
    "            m=m_smooth\n",
    "        )\n",
    "        X_tr = apply_target_mean_single(X_tr_raw, enc_map, prior).values.reshape(-1,1)\n",
    "        X_va = apply_target_mean_single(X_va_raw, enc_map, prior).values.reshape(-1,1)\n",
    "        X_tr[~np.isfinite(X_tr)] = prior\n",
    "        X_va[~np.isfinite(X_va)] = prior\n",
    "\n",
    "    # Check for constant features\n",
    "    if np.nanstd(X_tr) == 0 or np.nanstd(X_va) == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # Scale and fit\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    X_tr = scaler.fit_transform(X_tr)\n",
    "    X_va = scaler.transform(X_va)\n",
    "    \n",
    "    if np.isnan(X_tr).any() or np.isnan(X_va).any():\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    clf = LogisticRegression(solver=\"liblinear\", max_iter=200)\n",
    "    clf.fit(X_tr, y[tr_idx])\n",
    "    \n",
    "    p_tr = clf.predict_proba(X_tr)[:,1]\n",
    "    p_va = clf.predict_proba(X_va)[:,1]\n",
    "\n",
    "    try:   \n",
    "        auc_tr = roc_auc_score(y[tr_idx], p_tr)\n",
    "    except: \n",
    "        auc_tr = np.nan\n",
    "    try:   \n",
    "        auc_va = roc_auc_score(y[va_idx], p_va)\n",
    "    except: \n",
    "        auc_va = np.nan\n",
    "\n",
    "    return auc_tr, auc_va\n",
    "\n",
    "def time_consistency_scan(\n",
    "    train_df: pd.DataFrame,\n",
    "    candidate_cols: list,\n",
    "    folds: list,\n",
    "    m_smooth: float = 100.0,\n",
    "    verbose_every: int = 50\n",
    "):\n",
    "    \"\"\"Run time consistency scan using walk-forward folds\"\"\"\n",
    "    if TARGET not in train_df.columns:\n",
    "        raise KeyError(f\"Expected target '{TARGET}' in train_df\")\n",
    "\n",
    "    print(f\"Time consistency scan using {len(folds)} walk-forward folds\")\n",
    "\n",
    "    rows = []\n",
    "    for i, col in enumerate(candidate_cols, 1):\n",
    "        auc_trs, auc_vas = [], []\n",
    "        for (tr_idx, va_idx) in folds:\n",
    "            auc_tr, auc_va = fit_eval_single_feature(train_df, col, tr_idx, va_idx, m_smooth=m_smooth)\n",
    "            auc_trs.append(auc_tr)\n",
    "            auc_vas.append(auc_va)\n",
    "\n",
    "        row = {\n",
    "            \"feature\": col,\n",
    "            \"folds\": len(folds),\n",
    "            \"val_auc_mean\": np.nanmean(auc_vas),\n",
    "            \"val_auc_median\": np.nanmedian(auc_vas),\n",
    "            \"val_auc_std\": np.nanstd(auc_vas),\n",
    "            \"train_auc_mean\": np.nanmean(auc_trs),\n",
    "            \"gap_train_minus_val\": (np.nanmean(auc_trs) - np.nanmean(auc_vas)),\n",
    "            \"whitelist_flag\": False,\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "        if verbose_every and (i % verbose_every == 0):\n",
    "            print(f\"[{i}/{len(candidate_cols)}] {col} | val_mean={row['val_auc_mean']:.3f} gap={row['gap_train_minus_val']:.3f}\")\n",
    "\n",
    "    res = (pd.DataFrame(rows)\n",
    "           .sort_values([\"val_auc_mean\", \"gap_train_minus_val\"], ascending=[False, True])\n",
    "           .reset_index(drop=True))\n",
    "\n",
    "    # Mark unstable features\n",
    "    res[\"unstable_flag\"] = (res[\"val_auc_mean\"] < 0.50) | (res[\"gap_train_minus_val\"] > 0.10)\n",
    "\n",
    "    # Add whitelist features\n",
    "    wl_present = sorted([c for c in WHITELIST if c in train_df.columns])\n",
    "    wl_df = pd.DataFrame({\n",
    "        \"feature\": wl_present,\n",
    "        \"folds\": 0,\n",
    "        \"val_auc_mean\": np.nan,\n",
    "        \"val_auc_median\": np.nan, \n",
    "        \"val_auc_std\": np.nan,\n",
    "        \"train_auc_mean\": np.nan,\n",
    "        \"gap_train_minus_val\": np.nan,\n",
    "        \"whitelist_flag\": True,\n",
    "        \"unstable_flag\": False,\n",
    "    })\n",
    "\n",
    "    # Save results\n",
    "    os.makedirs(\"../reports\", exist_ok=True)\n",
    "    res_all = pd.concat([res, wl_df], ignore_index=True)\n",
    "    res_all.to_csv(\"../reports/time_consistency_results.csv\", index=False)\n",
    "\n",
    "    stable = res_all.loc[res_all[\"unstable_flag\"] == False, [\"feature\",\"whitelist_flag\"]].sort_values(\"feature\")\n",
    "    unstable = res_all.loc[(res_all[\"unstable_flag\"] == True) & (res_all[\"whitelist_flag\"] == False), [\"feature\"]].sort_values(\"feature\")\n",
    "\n",
    "    stable.to_csv(\"../reports/stable_features.csv\", index=False)\n",
    "    unstable.to_csv(\"../reports/unstable_features.csv\", index=False)\n",
    "\n",
    "    # Final feature list\n",
    "    keep_final = stable[\"feature\"].tolist()\n",
    "    pd.DataFrame({\"feature\": keep_final}).to_csv(\"../reports/keep_features_final.csv\", index=False)\n",
    "\n",
    "    print(f\"Results saved:\")\n",
    "    print(f\"  - Stable features: {len(stable)}\")\n",
    "    print(f\"  - Unstable features: {len(unstable)}\")\n",
    "    print(f\"  - Final keep list: {len(keep_final)}\")\n",
    "    \n",
    "    return res_all, stable, unstable\n",
    "\n",
    "# Run consistency scan\n",
    "candidates = build_scan_candidates(train_df)\n",
    "results_df, stable_df, unstable_df = time_consistency_scan(\n",
    "    train_df=train_df,\n",
    "    candidate_cols=candidates,\n",
    "    folds=folds,\n",
    "    m_smooth=100.0,\n",
    "    verbose_every=50\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 stable features by validation AUC:\")\n",
    "print(results_df.head(10)[[\"feature\", \"val_auc_mean\", \"gap_train_minus_val\", \"unstable_flag\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4a9dc7",
   "metadata": {},
   "source": [
    "## 9. Final Model-Ready Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b98c401e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final features:\n",
      "  - Train: 241 (including target)\n",
      "  - Test: 240\n",
      "Saved processed data to 'data/processed/' directory.\n"
     ]
    }
   ],
   "source": [
    "def build_final_cols(df_cols):\n",
    "    cols_to_keep = set()\n",
    "    for col in df_cols:\n",
    "        col_lower = col.lower()\n",
    "        # Keep base features\n",
    "        if col_lower in REQUIRED_BASE:\n",
    "            cols_to_keep.add(col)\n",
    "        # Keep features from our engineering\n",
    "        if any(col_lower.endswith(s) for s in INCLUDE_SUFFIXES):\n",
    "            cols_to_keep.add(col)\n",
    "        # Keep all features we created in the add_agg_features step\n",
    "        # by checking for the uid or agg_stats suffixes\n",
    "        if 'uid' in col_lower or 'amt' in col_lower or 'dt' in col_lower or 'devabs' in col_lower:\n",
    "             cols_to_keep.add(col)\n",
    "        # Keep original columns that aren't V-columns, IDs, or D-columns\n",
    "        # NOTE: Corrected to include 'v' and 'id_' prefixes\n",
    "        if any(col_lower.startswith(p) for p in ['card', 'addr', 'p_', 'r_', 'm', 'v', 'id_']):\n",
    "            cols_to_keep.add(col)\n",
    "        if 'productcd' in col_lower:\n",
    "            cols_to_keep.add(col)\n",
    "    \n",
    "    # Exclude raw UIDs if their encoded versions exist\n",
    "    raw_uids = {\"uid1\", \"uid2\", \"uid3\", \"uid4\"}\n",
    "    final_keep = sorted(list(cols_to_keep - raw_uids))\n",
    "    \n",
    "    return final_keep\n",
    "\n",
    "# Align train/test (except target)\n",
    "final_train_cols_full = build_final_cols(train_df.columns)\n",
    "final_test_cols_full = build_final_cols(test_df.columns)\n",
    "\n",
    "final_common = sorted(list(set(final_train_cols_full) & set(final_test_cols_full)))\n",
    "final_train_cols_aligned = [TARGET] + final_common if TARGET in train_df.columns else final_common\n",
    "final_test_cols_aligned = final_common\n",
    "\n",
    "print(f\"Final features:\")\n",
    "print(f\"  - Train: {len(final_train_cols_aligned)} (including target)\")\n",
    "print(f\"  - Test: {len(final_test_cols_aligned)}\")\n",
    "\n",
    "train_out = train_df[final_train_cols_aligned].copy()\n",
    "test_out = test_df[final_test_cols_aligned].copy()\n",
    "\n",
    "train_out.to_csv(OUTDIR + \"train_processed.csv\", index=False)\n",
    "test_out.to_csv(OUTDIR + \"test_processed.csv\", index=False)\n",
    "print(\"Saved processed data to 'data/processed/' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0fd112",
   "metadata": {},
   "source": [
    "## 10. Feature Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b7ec44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE ENGINEERING PIPELINE COMPLETE ===\n",
      "\n",
      "Features created:\n",
      "1. ✅ Base primitives: time, amount, email domains\n",
      "2. ✅ V-column reduction: 339 -> 128 columns\n",
      "3. ✅ UID aggregates: time deltas, amount stats, frequency\n",
      "4. ✅ Walk-forward target encoding (6 months)\n",
      "5. ✅ Time consistency testing\n",
      "6. ✅ Final stable feature selection\n",
      "\n",
      "Datasets ready for modeling:\n",
      "- Training: 590,540 rows × 241 features\n",
      "- Test: 506,691 rows × 240 features\n",
      "\n",
      "Key feature categories included:\n",
      "  - Basic: 2 features\n",
      "  - Card/Addr: 24 features\n",
      "  - Email: 6 features\n",
      "  - Device: 7 features\n",
      "  - UID Stats: 10 features\n",
      "  - Target Encoded: 17 features\n",
      "  - Frequency: 17 features\n",
      "  - V Reduced: 128 features\n",
      "\n",
      "Reports generated:\n",
      "  - ../reports/time_consistency_results.csv\n",
      "  - ../reports/stable_features.csv\n",
      "  - ../reports/unstable_features.csv\n",
      "  - ../reports/keep_features_final.csv\n",
      "\n",
      "Next steps:\n",
      "1. Train models on ../data/processed/train_model.csv\n",
      "2. Use walk-forward folds for validation\n",
      "3. Generate predictions on ../data/processed/test_model.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_transaction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m3. Generate predictions on ../data/processed/test_model.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Memory cleanup\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mtrain_transaction\u001b[49m, train_identity, test_transaction, test_identity\n\u001b[32m     45\u001b[39m gc.collect()\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Pipeline complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_transaction' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=== FEATURE ENGINEERING PIPELINE COMPLETE ===\\n\")\n",
    "\n",
    "print(\"Features created:\")\n",
    "print(\"1. ✅ Base primitives: time, amount, email domains\")\n",
    "print(\"2. ✅ V-column reduction: 339 -> 128 columns\")\n",
    "print(\"3. ✅ UID aggregates: time deltas, amount stats, frequency\")\n",
    "print(\"4. ✅ Walk-forward target encoding (6 months)\")\n",
    "print(\"5. ✅ Time consistency testing\")\n",
    "print(\"6. ✅ Final stable feature selection\")\n",
    "\n",
    "print(f\"\\nDatasets ready for modeling:\")\n",
    "print(f\"- Training: {train_out.shape[0]:,} rows × {train_out.shape[1]} features\")\n",
    "print(f\"- Test: {test_out.shape[0]:,} rows × {test_out.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nKey feature categories included:\")\n",
    "feature_categories = {\n",
    "    \"Basic\": [c for c in final_common if c in [\"transactionamt\", \"log_amt\", \"hour\", \"dow\"]],\n",
    "    \"Card/Addr\": [c for c in final_common if c.startswith((\"card\", \"addr\"))],\n",
    "    \"Email\": [c for c in final_common if \"email\" in c or c.endswith(\"_ed_parent\")],\n",
    "    \"Device\": [c for c in final_common if c.startswith((\"device\", \"id_30\", \"id_31\", \"res_\"))],\n",
    "    \"UID Stats\": [c for c in final_common if \"uid2_\" in c or \"uid3_\" in c],\n",
    "    \"Target Encoded\": [c for c in final_common if c.endswith(\"_te_wf\")],\n",
    "    \"Frequency\": [c for c in final_common if c.endswith(\"_fe\")],\n",
    "    \"V Reduced\": [c for c in final_common if c.startswith(\"v\")],\n",
    "    \"Other\": []\n",
    "}\n",
    "\n",
    "for cat, cols in feature_categories.items():\n",
    "    if cols:\n",
    "        print(f\"  - {cat}: {len(cols)} features\")\n",
    "\n",
    "print(f\"\\nReports generated:\")\n",
    "print(f\"  - ../reports/time_consistency_results.csv\")\n",
    "print(f\"  - ../reports/stable_features.csv\")  \n",
    "print(f\"  - ../reports/unstable_features.csv\")\n",
    "print(f\"  - ../reports/keep_features_final.csv\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"1. Train models on ../data/processed/train_model.csv\")\n",
    "print(f\"2. Use walk-forward folds for validation\")\n",
    "print(f\"3. Generate predictions on ../data/processed/test_model.csv\")\n",
    "\n",
    "# Memory cleanup\n",
    "del train_transaction, train_identity, test_transaction, test_identity\n",
    "gc.collect()\n",
    "print(f\"\\n✅ Pipeline complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60426f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
