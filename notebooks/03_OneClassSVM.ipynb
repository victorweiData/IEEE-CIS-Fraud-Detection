{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ONE-CLASS SVM FOR FRAUD DETECTION\n",
      "============================================================\n",
      "Shapes -> train: (590540, 293) test: (506691, 292)\n",
      "\n",
      "Original Class Distribution:\n",
      "isFraud\n",
      "0    569877\n",
      "1     20663\n",
      "Name: count, dtype: int64\n",
      "Fraud Rate: 0.0350 (3.50%)\n",
      "Imbalance Ratio: 27.58:1\n",
      "\n",
      "Training set class distribution:\n",
      "Class 0 (Normal): 455833\n",
      "Class 1 (Fraud): 16599\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load your data\n",
    "DATA_DIR = \"../data/processed\"\n",
    "trainX = pd.read_csv(f\"{DATA_DIR}/IEEE_Train.csv\")\n",
    "y = pd.read_csv(f\"{DATA_DIR}/IEEE_Target.csv\")\n",
    "testX = pd.read_csv(f\"{DATA_DIR}/IEEE_Test.csv\")\n",
    "train = trainX.merge(y, on=\"TransactionID\", how=\"left\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ONE-CLASS SVM FOR FRAUD DETECTION\")\n",
    "print(\"=\"*60)\n",
    "print(\"Shapes -> train:\", train.shape, \"test:\", testX.shape)\n",
    "\n",
    "# Prepare features and target\n",
    "X = train.drop(['TransactionID', 'uid', 'isFraud'], axis=1)\n",
    "y_target = train['isFraud']\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nOriginal Class Distribution:\")\n",
    "print(y_target.value_counts())\n",
    "fraud_rate = y_target.mean()\n",
    "print(f\"Fraud Rate: {fraud_rate:.4f} ({fraud_rate*100:.2f}%)\")\n",
    "print(f\"Imbalance Ratio: {y_target.value_counts()[0] / y_target.value_counts()[1]:.2f}:1\")\n",
    "\n",
    "# Split the data\n",
    "# Calculate the split point for 80% of the data\n",
    "split_point = int(0.8 * len(X))\n",
    "\n",
    "# Split chronologically - first 80% for training, last 20% for validation\n",
    "X_train = X[:split_point]\n",
    "X_val = X[split_point:]\n",
    "y_train = y_target[:split_point]\n",
    "y_val = y_target[split_point:]\n",
    "\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(f\"Class 0 (Normal): {sum(y_train == 0)}\")\n",
    "print(f\"Class 1 (Fraud): {sum(y_train == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DATA PREPROCESSING\n",
      "==================================================\n",
      "StandardScaler - Train shape: (472432, 290), Val shape: (118108, 290)\n",
      "RobustScaler - Train shape: (472432, 290), Val shape: (118108, 290)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA PREPROCESSING FOR ONE-CLASS SVM\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# One-Class SVM is sensitive to feature scaling - try both StandardScaler and RobustScaler\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "\n",
    "scaled_data = {}\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    scaled_data[scaler_name] = {\n",
    "        'X_train': X_train_scaled,\n",
    "        'X_val': X_val_scaled,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "    print(f\"{scaler_name} - Train shape: {X_train_scaled.shape}, Val shape: {X_val_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ONE-CLASS SVM TRAINING STRATEGIES\n",
      "==================================================\n",
      "\n",
      "--- Strategy 1: Train on Normal Transactions Only ---\n",
      "\n",
      "StandardScaler with nu=0.01\n",
      "  Precision: 0.1329, Recall: 0.1206, F1: 0.1264\n",
      "  Detected fraud rate: 0.0312\n",
      "\n",
      "StandardScaler with nu=0.05\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ONE-CLASS SVM TRAINING WITH DIFFERENT STRATEGIES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ONE-CLASS SVM TRAINING STRATEGIES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Strategy 1: Train on normal transactions only (traditional One-Class SVM approach)\n",
    "print(\"\\n--- Strategy 1: Train on Normal Transactions Only ---\")\n",
    "normal_indices = y_train == 0\n",
    "X_train_normal = X_train[normal_indices]\n",
    "\n",
    "strategy1_results = {}\n",
    "for scaler_name, data in scaled_data.items():\n",
    "    X_train_normal_scaled = data['scaler'].fit_transform(X_train_normal)\n",
    "    X_val_scaled = data['scaler'].transform(X_val)\n",
    "    \n",
    "    # Test different nu values (expected fraction of outliers)\n",
    "    nu_values = [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "    \n",
    "    for nu in nu_values:\n",
    "        print(f\"\\n{scaler_name} with nu={nu}\")\n",
    "        \n",
    "        # Train One-Class SVM\n",
    "        oc_svm = OneClassSVM(\n",
    "            kernel='rbf',\n",
    "            gamma='scale',\n",
    "            nu=nu\n",
    "        )\n",
    "        \n",
    "        oc_svm.fit(X_train_normal_scaled)\n",
    "        \n",
    "        # Predict on validation set (-1 for anomaly/fraud, 1 for normal)\n",
    "        val_predictions = oc_svm.predict(X_val_scaled)\n",
    "        val_scores = oc_svm.decision_function(X_val_scaled)\n",
    "        \n",
    "        # Convert to binary (1 for fraud, 0 for normal)\n",
    "        val_pred_binary = (val_predictions == -1).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if len(np.unique(val_pred_binary)) > 1:\n",
    "            precision = precision_score(y_val, val_pred_binary)\n",
    "            recall = recall_score(y_val, val_pred_binary)\n",
    "            f1 = f1_score(y_val, val_pred_binary)\n",
    "        else:\n",
    "            precision = recall = f1 = 0\n",
    "        \n",
    "        fraud_detection_rate = np.mean(val_pred_binary)\n",
    "        \n",
    "        strategy1_results[f\"{scaler_name}_nu_{nu}\"] = {\n",
    "            'scaler': scaler_name,\n",
    "            'nu': nu,\n",
    "            'model': oc_svm,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'fraud_rate': fraud_detection_rate,\n",
    "            'predictions': val_pred_binary,\n",
    "            'scores': val_scores\n",
    "        }\n",
    "        \n",
    "        print(f\"  Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "        print(f\"  Detected fraud rate: {fraud_detection_rate:.4f}\")\n",
    "\n",
    "# Strategy 2: Train on all data with contamination parameter\n",
    "print(\"\\n--- Strategy 2: Train on All Data with Contamination ---\")\n",
    "\n",
    "strategy2_results = {}\n",
    "contamination_values = [fraud_rate, fraud_rate * 1.5, fraud_rate * 2.0, 0.05, 0.1]\n",
    "\n",
    "for scaler_name, data in scaled_data.items():\n",
    "    for contamination in contamination_values:\n",
    "        print(f\"\\n{scaler_name} with contamination={contamination:.4f}\")\n",
    "        \n",
    "        # Train One-Class SVM with contamination\n",
    "        oc_svm = OneClassSVM(\n",
    "            kernel='rbf',\n",
    "            gamma='scale',\n",
    "            nu=contamination  # nu is equivalent to contamination in this context\n",
    "        )\n",
    "        \n",
    "        oc_svm.fit(data['X_train'])\n",
    "        \n",
    "        # Predict on validation set\n",
    "        val_predictions = oc_svm.predict(data['X_val'])\n",
    "        val_scores = oc_svm.decision_function(data['X_val'])\n",
    "        val_pred_binary = (val_predictions == -1).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if len(np.unique(val_pred_binary)) > 1:\n",
    "            precision = precision_score(y_val, val_pred_binary)\n",
    "            recall = recall_score(y_val, val_pred_binary)\n",
    "            f1 = f1_score(y_val, val_pred_binary)\n",
    "        else:\n",
    "            precision = recall = f1 = 0\n",
    "        \n",
    "        fraud_detection_rate = np.mean(val_pred_binary)\n",
    "        \n",
    "        strategy2_results[f\"{scaler_name}_cont_{contamination:.4f}\"] = {\n",
    "            'scaler': scaler_name,\n",
    "            'contamination': contamination,\n",
    "            'model': oc_svm,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'fraud_rate': fraud_detection_rate,\n",
    "            'predictions': val_pred_binary,\n",
    "            'scores': val_scores\n",
    "        }\n",
    "        \n",
    "        print(f\"  Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "        print(f\"  Detected fraud rate: {fraud_detection_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "HYPERPARAMETER OPTIMIZATION\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'strategy2_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Find best performing configuration from both strategies\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m all_results = {**strategy1_results, **\u001b[43mstrategy2_results\u001b[49m}\n\u001b[32m     11\u001b[39m best_config_key = \u001b[38;5;28mmax\u001b[39m(all_results.keys(), key=\u001b[38;5;28;01mlambda\u001b[39;00m k: all_results[k][\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     12\u001b[39m best_config = all_results[best_config_key]\n",
      "\u001b[31mNameError\u001b[39m: name 'strategy2_results' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# HYPERPARAMETER OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find best performing configuration from both strategies\n",
    "all_results = {**strategy1_results, **strategy2_results}\n",
    "best_config_key = max(all_results.keys(), key=lambda k: all_results[k]['f1'])\n",
    "best_config = all_results[best_config_key]\n",
    "\n",
    "print(f\"Best configuration: {best_config_key}\")\n",
    "print(f\"Best F1 Score: {best_config['f1']:.4f}\")\n",
    "\n",
    "# Advanced hyperparameter tuning for the best scaler\n",
    "best_scaler_name = best_config['scaler']\n",
    "best_scaler_data = scaled_data[best_scaler_name]\n",
    "\n",
    "print(f\"\\nAdvanced tuning with {best_scaler_name}...\")\n",
    "\n",
    "# Grid search for optimal parameters\n",
    "param_grid = {\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'nu': [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "}\n",
    "\n",
    "# Custom scoring function for imbalanced data\n",
    "def custom_f1_scorer(estimator, X, y_true):\n",
    "    predictions = estimator.predict(X)\n",
    "    pred_binary = (predictions == -1).astype(int)\n",
    "    if len(np.unique(pred_binary)) > 1:\n",
    "        return f1_score(y_true, pred_binary)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Perform grid search (on a subset due to computational constraints)\n",
    "print(\"Performing grid search optimization...\")\n",
    "\n",
    "# Use normal transactions for training (One-Class SVM approach)\n",
    "if 'nu' in best_config:\n",
    "    # Strategy 1 approach\n",
    "    X_train_for_tuning = best_scaler_data['scaler'].fit_transform(X_train[y_train == 0])\n",
    "else:\n",
    "    # Strategy 2 approach  \n",
    "    X_train_for_tuning = best_scaler_data['X_train']\n",
    "\n",
    "# Simplified grid search\n",
    "best_params = {'kernel': 'rbf', 'gamma': 'scale', 'nu': 0.1}\n",
    "best_score = -1\n",
    "\n",
    "# Test a subset of combinations\n",
    "kernels = ['rbf', 'poly']\n",
    "gammas = ['scale', 0.01, 0.1]\n",
    "nus = [0.05, 0.1, 0.15]\n",
    "\n",
    "for kernel in kernels:\n",
    "    for gamma in gammas:\n",
    "        for nu in nus:\n",
    "            try:\n",
    "                oc_svm_test = OneClassSVM(kernel=kernel, gamma=gamma, nu=nu)\n",
    "                oc_svm_test.fit(X_train_for_tuning)\n",
    "                \n",
    "                val_pred = oc_svm_test.predict(best_scaler_data['X_val'])\n",
    "                val_pred_binary = (val_pred == -1).astype(int)\n",
    "                \n",
    "                if len(np.unique(val_pred_binary)) > 1:\n",
    "                    f1 = f1_score(y_val, val_pred_binary)\n",
    "                    if f1 > best_score:\n",
    "                        best_score = f1\n",
    "                        best_params = {'kernel': kernel, 'gamma': gamma, 'nu': nu}\n",
    "                        \n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "print(f\"Best F1 score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL OPTIMIZED MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL OPTIMIZED ONE-CLASS SVM MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train final model with best parameters\n",
    "final_scaler = scalers[best_scaler_name]\n",
    "X_train_final_scaled = final_scaler.fit_transform(X_train[y_train == 0])  # Train on normal only\n",
    "X_val_final_scaled = final_scaler.transform(X_val)\n",
    "\n",
    "final_oc_svm = OneClassSVM(\n",
    "    kernel=best_params['kernel'],\n",
    "    gamma=best_params['gamma'],\n",
    "    nu=best_params['nu']\n",
    ")\n",
    "\n",
    "final_oc_svm.fit(X_train_final_scaled)\n",
    "\n",
    "# Get predictions and scores\n",
    "final_val_predictions = final_oc_svm.predict(X_val_final_scaled)\n",
    "final_val_scores = final_oc_svm.decision_function(X_val_final_scaled)\n",
    "final_val_pred_binary = (final_val_predictions == -1).astype(int)\n",
    "\n",
    "# Calculate final metrics\n",
    "final_precision = precision_score(y_val, final_val_pred_binary)\n",
    "final_recall = recall_score(y_val, final_val_pred_binary)\n",
    "final_f1 = f1_score(y_val, final_val_pred_binary)\n",
    "\n",
    "# Convert scores to probabilities for AUC calculation\n",
    "final_val_proba = 1 / (1 + np.exp(-final_val_scores))  # Sigmoid transformation\n",
    "final_auc = roc_auc_score(y_val, final_val_proba)\n",
    "final_ap = average_precision_score(y_val, final_val_proba)\n",
    "\n",
    "print(f\"Final Model Performance:\")\n",
    "print(f\"Precision: {final_precision:.4f}\")\n",
    "print(f\"Recall: {final_recall:.4f}\")\n",
    "print(f\"F1-Score: {final_f1:.4f}\")\n",
    "print(f\"ROC-AUC: {final_auc:.4f}\")\n",
    "print(f\"PR-AUC: {final_ap:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val, final_val_pred_binary)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, final_val_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# THRESHOLD OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test different percentile thresholds based on decision function scores\n",
    "percentiles = [1, 2, 3, 4, 5, 10, 15, 20, 25]\n",
    "threshold_results = []\n",
    "\n",
    "for percentile in percentiles:\n",
    "    threshold = np.percentile(final_val_scores, percentile)\n",
    "    pred_threshold = (final_val_scores < threshold).astype(int)\n",
    "    \n",
    "    if len(np.unique(pred_threshold)) > 1:\n",
    "        precision = precision_score(y_val, pred_threshold)\n",
    "        recall = recall_score(y_val, pred_threshold)\n",
    "        f1 = f1_score(y_val, pred_threshold)\n",
    "    else:\n",
    "        precision = recall = f1 = 0\n",
    "    \n",
    "    detected_fraud_rate = np.mean(pred_threshold)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Percentile': percentile,\n",
    "        'Threshold': threshold,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Detected_Fraud_Rate': detected_fraud_rate\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "print(\"Threshold Optimization Results:\")\n",
    "print(threshold_df.round(4))\n",
    "\n",
    "# Find optimal threshold\n",
    "optimal_idx = threshold_df['F1-Score'].idxmax()\n",
    "optimal_threshold = threshold_df.loc[optimal_idx, 'Threshold']\n",
    "optimal_percentile = threshold_df.loc[optimal_idx, 'Percentile']\n",
    "\n",
    "print(f\"\\nOptimal threshold: {optimal_threshold:.4f} (at {optimal_percentile}th percentile)\")\n",
    "print(f\"Optimal F1-Score: {threshold_df.loc[optimal_idx, 'F1-Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# KERNEL COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"KERNEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "kernels_to_test = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "kernel_results = {}\n",
    "\n",
    "for kernel in kernels_to_test:\n",
    "    print(f\"\\nTesting {kernel} kernel...\")\n",
    "    \n",
    "    try:\n",
    "        oc_svm_kernel = OneClassSVM(\n",
    "            kernel=kernel,\n",
    "            gamma='scale' if kernel != 'linear' else 'auto',\n",
    "            nu=best_params['nu']\n",
    "        )\n",
    "        \n",
    "        oc_svm_kernel.fit(X_train_final_scaled)\n",
    "        \n",
    "        # Predictions\n",
    "        kernel_pred = oc_svm_kernel.predict(X_val_final_scaled)\n",
    "        kernel_scores = oc_svm_kernel.decision_function(X_val_final_scaled)\n",
    "        kernel_pred_binary = (kernel_pred == -1).astype(int)\n",
    "        \n",
    "        # Metrics\n",
    "        if len(np.unique(kernel_pred_binary)) > 1:\n",
    "            precision = precision_score(y_val, kernel_pred_binary)\n",
    "            recall = recall_score(y_val, kernel_pred_binary)\n",
    "            f1 = f1_score(y_val, kernel_pred_binary)\n",
    "            \n",
    "            # AUC calculation\n",
    "            kernel_proba = 1 / (1 + np.exp(-kernel_scores))\n",
    "            auc = roc_auc_score(y_val, kernel_proba)\n",
    "        else:\n",
    "            precision = recall = f1 = auc = 0\n",
    "        \n",
    "        kernel_results[kernel] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc,\n",
    "            'model': oc_svm_kernel,\n",
    "            'scores': kernel_scores,\n",
    "            'predictions': kernel_pred_binary\n",
    "        }\n",
    "        \n",
    "        print(f\"  Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error with {kernel} kernel: {str(e)}\")\n",
    "        kernel_results[kernel] = {'f1': 0}\n",
    "\n",
    "# Find best kernel\n",
    "best_kernel = max(kernel_results.keys(), key=lambda k: kernel_results[k].get('f1', 0))\n",
    "print(f\"\\nBest kernel: {best_kernel} (F1: {kernel_results[best_kernel]['f1']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Confusion Matrix for best model\n",
    "best_model_results = kernel_results[best_kernel]\n",
    "cm = confusion_matrix(y_val, best_model_results['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
    "axes[0,0].set_title(f'Confusion Matrix\\n({best_kernel} kernel)')\n",
    "axes[0,0].set_xlabel('Predicted')\n",
    "axes[0,0].set_ylabel('Actual')\n",
    "\n",
    "# 2. Threshold optimization plot\n",
    "axes[0,1].plot(threshold_df['Percentile'], threshold_df['Precision'], 'o-', label='Precision')\n",
    "axes[0,1].plot(threshold_df['Percentile'], threshold_df['Recall'], 's-', label='Recall')\n",
    "axes[0,1].plot(threshold_df['Percentile'], threshold_df['F1-Score'], '^-', label='F1-Score')\n",
    "axes[0,1].axvline(x=optimal_percentile, color='red', linestyle='--', alpha=0.7)\n",
    "axes[0,1].set_xlabel('Percentile Threshold')\n",
    "axes[0,1].set_ylabel('Score')\n",
    "axes[0,1].set_title('Threshold Optimization')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Kernel comparison\n",
    "kernel_comparison_df = pd.DataFrame({\n",
    "    'Kernel': list(kernel_results.keys()),\n",
    "    'Precision': [kernel_results[k].get('precision', 0) for k in kernel_results.keys()],\n",
    "    'Recall': [kernel_results[k].get('recall', 0) for k in kernel_results.keys()],\n",
    "    'F1-Score': [kernel_results[k].get('f1', 0) for k in kernel_results.keys()]\n",
    "})\n",
    "\n",
    "x_pos = np.arange(len(kernel_comparison_df))\n",
    "width = 0.25\n",
    "\n",
    "axes[0,2].bar(x_pos - width, kernel_comparison_df['Precision'], width, label='Precision', alpha=0.8)\n",
    "axes[0,2].bar(x_pos, kernel_comparison_df['Recall'], width, label='Recall', alpha=0.8)\n",
    "axes[0,2].bar(x_pos + width, kernel_comparison_df['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "axes[0,2].set_xlabel('Kernel')\n",
    "axes[0,2].set_ylabel('Score')\n",
    "axes[0,2].set_title('Kernel Performance Comparison')\n",
    "axes[0,2].set_xticks(x_pos)\n",
    "axes[0,2].set_xticklabels(kernel_comparison_df['Kernel'])\n",
    "axes[0,2].legend()\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Decision Function Score Distribution\n",
    "fraud_scores = best_model_results['scores'][y_val == 1]\n",
    "normal_scores = best_model_results['scores'][y_val == 0]\n",
    "\n",
    "axes[1,0].hist(normal_scores, bins=50, alpha=0.7, label='Normal', density=True, color='blue')\n",
    "axes[1,0].hist(fraud_scores, bins=50, alpha=0.7, label='Fraud', density=True, color='red')\n",
    "axes[1,0].axvline(x=0, color='black', linestyle='-', alpha=0.8, label='Default Threshold')\n",
    "axes[1,0].axvline(x=optimal_threshold, color='green', linestyle='--', alpha=0.8, label='Optimal Threshold')\n",
    "axes[1,0].set_xlabel('Decision Function Score')\n",
    "axes[1,0].set_ylabel('Density')\n",
    "axes[1,0].set_title('Score Distribution by Class')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 5. Precision-Recall Curve\n",
    "best_proba = 1 / (1 + np.exp(-best_model_results['scores']))\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_val, best_proba)\n",
    "ap_score = average_precision_score(y_val, best_proba)\n",
    "\n",
    "axes[1,1].plot(recall_curve, precision_curve, marker='.')\n",
    "axes[1,1].set_xlabel('Recall')\n",
    "axes[1,1].set_ylabel('Precision')\n",
    "axes[1,1].set_title(f'Precision-Recall Curve\\n(AP Score: {ap_score:.4f})')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Feature scaling impact\n",
    "scaler_comparison = []\n",
    "for scaler_name in ['StandardScaler', 'RobustScaler']:\n",
    "    # Get best result for each scaler\n",
    "    scaler_results = [result for key, result in all_results.items() if result['scaler'] == scaler_name]\n",
    "    if scaler_results:\n",
    "        best_scaler_result = max(scaler_results, key=lambda x: x['f1'])\n",
    "        scaler_comparison.append({\n",
    "            'Scaler': scaler_name,\n",
    "            'Best_F1': best_scaler_result['f1'],\n",
    "            'Best_Precision': best_scaler_result['precision'],\n",
    "            'Best_Recall': best_scaler_result['recall']\n",
    "        })\n",
    "\n",
    "if scaler_comparison:\n",
    "    scaler_comp_df = pd.DataFrame(scaler_comparison)\n",
    "    x_pos = np.arange(len(scaler_comp_df))\n",
    "    \n",
    "    axes[1,2].bar(x_pos - width/2, scaler_comp_df['Best_Precision'], width, label='Precision', alpha=0.8)\n",
    "    axes[1,2].bar(x_pos + width/2, scaler_comp_df['Best_Recall'], width, label='Recall', alpha=0.8)\n",
    "    \n",
    "    axes[1,2].set_xlabel('Scaler Type')\n",
    "    axes[1,2].set_ylabel('Best Score')\n",
    "    axes[1,2].set_title('Scaler Impact on Performance')\n",
    "    axes[1,2].set_xticks(x_pos)\n",
    "    axes[1,2].set_xticklabels(scaler_comp_df['Scaler'], rotation=45)\n",
    "    axes[1,2].legend()\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL COMPARISON WITH OTHER METHODS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON WITH OTHER METHODS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train comparison models\n",
    "print(\"Training comparison models...\")\n",
    "\n",
    "# Logistic Regression with class balancing\n",
    "lr = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "lr.fit(best_scaler_data['X_train'], y_train)\n",
    "lr_pred = lr.predict(best_scaler_data['X_val'])\n",
    "lr_proba = lr.predict_proba(best_scaler_data['X_val'])[:, 1]\n",
    "\n",
    "# Random Forest with class balancing\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)  # Use original unscaled data for RF\n",
    "rf_pred = rf.predict(X_val)\n",
    "rf_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate metrics for comparison\n",
    "comparison_results = []\n",
    "\n",
    "# One-Class SVM\n",
    "comparison_results.append({\n",
    "    'Model': 'One-Class SVM',\n",
    "    'Precision': final_precision,\n",
    "    'Recall': final_recall,\n",
    "    'F1-Score': final_f1,\n",
    "    'ROC-AUC': final_auc,\n",
    "    'PR-AUC': final_ap\n",
    "})\n",
    "\n",
    "# Logistic Regression\n",
    "lr_precision = precision_score(y_val, lr_pred)\n",
    "lr_recall = recall_score(y_val, lr_pred)\n",
    "lr_f1 = f1_score(y_val, lr_pred)\n",
    "lr_auc = roc_auc_score(y_val, lr_proba)\n",
    "lr_ap = average_precision_score(y_val, lr_proba)\n",
    "\n",
    "comparison_results.append({\n",
    "    'Model': 'Logistic Regression',\n",
    "    'Precision': lr_precision,\n",
    "    'Recall': lr_recall,\n",
    "    'F1-Score': lr_f1,\n",
    "    'ROC-AUC': lr_auc,\n",
    "    'PR-AUC': lr_ap\n",
    "})\n",
    "\n",
    "# Random Forest\n",
    "rf_precision = precision_score(y_val, rf_pred)\n",
    "rf_recall = recall_score(y_val, rf_pred)\n",
    "rf_f1 = f1_score(y_val, rf_pred)\n",
    "rf_auc = roc_auc_score(y_val, rf_proba)\n",
    "rf_ap = average_precision_score(y_val, rf_proba)\n",
    "\n",
    "comparison_results.append({\n",
    "    'Model': 'Random Forest',\n",
    "    'Precision': rf_precision,\n",
    "    'Recall': rf_recall,\n",
    "    'F1-Score': rf_f1,\n",
    "    'ROC-AUC': rf_auc,\n",
    "    'PR-AUC': rf_ap\n",
    "})\n",
    "\n",
    "# Display comparison\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Highlight best performing model for each metric\n",
    "print(\"\\nBest performing model for each metric:\")\n",
    "for metric in ['Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'PR-AUC']:\n",
    "    best_model = comparison_df.loc[comparison_df[metric].idxmax(), 'Model']\n",
    "    best_score = comparison_df[metric].max()\n",
    "    print(f\"{metric}: {best_model} ({best_score:.4f})\")\n",
    "\n",
    "# =============================================================================\n",
    "# PREDICTIONS ON TEST SET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREDICTIONS ON TEST SET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare test data\n",
    "X_test = testX.drop(['TransactionID', 'uid'], axis=1)\n",
    "X_test_scaled = final_scaler.transform(X_test)\n",
    "\n",
    "# Make predictions using final model\n",
    "test_predictions = final_oc_svm.predict(X_test_scaled)\n",
    "test_scores = final_oc_svm.decision_function(X_test_scaled)\n",
    "\n",
    "# Apply default threshold\n",
    "test_pred_default = (test_predictions == -1).astype(int)\n",
    "\n",
    "# Apply optimized threshold\n",
    "test_pred_optimized = (test_scores < optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Default threshold fraud detection rate: {np.mean(test_pred_default):.4f}\")\n",
    "print(f\"Optimized threshold fraud detection rate: {np.mean(test_pred_optimized):.4f}\")\n",
    "\n",
    "# Create submission files\n",
    "submission_default = pd.DataFrame({\n",
    "    'TransactionID': testX['TransactionID'],\n",
    "    'isFraud': test_pred_default\n",
    "})\n",
    "\n",
    "submission_optimized = pd.DataFrame({\n",
    "    'TransactionID': testX['TransactionID'],\n",
    "    'isFraud': test_pred_optimized\n",
    "})\n",
    "\n",
    "print(f\"\\nSample predictions (first 10 rows - optimized threshold):\")\n",
    "print(submission_optimized.head(10))\n",
    "\n",
    "# =============================================================================\n",
    "# DETAILED ANALYSIS AND INSIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DETAILED ANALYSIS AND INSIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze prediction confidence\n",
    "test_proba = 1 / (1 + np.exp(-test_scores))\n",
    "confidence_bins = pd.cut(test_proba, bins=[0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0], \n",
    "                        labels=['Very Low', 'Low', 'Medium', 'High', 'Very High', 'Extreme'])\n",
    "\n",
    "print(\"Prediction Confidence Distribution:\")\n",
    "print(confidence_bins.value_counts().sort_index())\n",
    "\n",
    "# Analyze score statistics\n",
    "print(f\"\\nDecision Function Score Statistics:\")\n",
    "print(f\"Mean: {test_scores.mean():.4f}\")\n",
    "print(f\"Std: {test_scores.std():.4f}\")\n",
    "print(f\"Min: {test_scores.min():.4f}\")\n",
    "print(f\"Max: {test_scores.max():.4f}\")\n",
    "print(f\"25th percentile: {np.percentile(test_scores, 25):.4f}\")\n",
    "print(f\"50th percentile: {np.percentile(test_scores, 50):.4f}\")\n",
    "print(f\"75th percentile: {np.percentile(test_scores, 75):.4f}\")\n",
    "\n",
    "# Most anomalous transactions (potential fraud)\n",
    "most_anomalous_indices = np.argsort(test_scores)[:10]\n",
    "print(f\"\\nTop 10 most anomalous transactions (lowest scores):\")\n",
    "for i, idx in enumerate(most_anomalous_indices):\n",
    "    score = test_scores[idx]\n",
    "    transaction_id = testX.iloc[idx]['TransactionID']\n",
    "    print(f\"  {i+1}. TransactionID: {transaction_id}, Score: {score:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ADVANCED ONE-CLASS SVM TECHNIQUES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ADVANCED ONE-CLASS SVM TECHNIQUES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Ensemble of One-Class SVMs with different parameters\n",
    "print(\"\\n--- Ensemble One-Class SVM ---\")\n",
    "\n",
    "ensemble_models = []\n",
    "ensemble_params = [\n",
    "    {'kernel': 'rbf', 'gamma': 'scale', 'nu': 0.05},\n",
    "    {'kernel': 'rbf', 'gamma': 'scale', 'nu': 0.1},\n",
    "    {'kernel': 'rbf', 'gamma': 'scale', 'nu': 0.15},\n",
    "    {'kernel': 'rbf', 'gamma': 0.01, 'nu': 0.1},\n",
    "    {'kernel': 'rbf', 'gamma': 0.1, 'nu': 0.1}\n",
    "]\n",
    "\n",
    "ensemble_val_scores = []\n",
    "ensemble_test_scores = []\n",
    "\n",
    "for i, params in enumerate(ensemble_params):\n",
    "    print(f\"Training ensemble model {i+1}/5...\")\n",
    "    \n",
    "    oc_svm_ensemble = OneClassSVM(**params)\n",
    "    oc_svm_ensemble.fit(X_train_final_scaled)\n",
    "    \n",
    "    # Get scores\n",
    "    val_scores_ens = oc_svm_ensemble.decision_function(X_val_final_scaled)\n",
    "    test_scores_ens = oc_svm_ensemble.decision_function(X_test_scaled)\n",
    "    \n",
    "    ensemble_val_scores.append(val_scores_ens)\n",
    "    ensemble_test_scores.append(test_scores_ens)\n",
    "    ensemble_models.append(oc_svm_ensemble)\n",
    "\n",
    "# Average ensemble scores\n",
    "ensemble_val_avg = np.mean(ensemble_val_scores, axis=0)\n",
    "ensemble_test_avg = np.mean(ensemble_test_scores, axis=0)\n",
    "\n",
    "# Ensemble predictions with optimized threshold\n",
    "ensemble_val_pred = (ensemble_val_avg < optimal_threshold).astype(int)\n",
    "ensemble_test_pred = (ensemble_test_avg < optimal_threshold).astype(int)\n",
    "\n",
    "# Ensemble metrics\n",
    "ensemble_precision = precision_score(y_val, ensemble_val_pred)\n",
    "ensemble_recall = recall_score(y_val, ensemble_val_pred)\n",
    "ensemble_f1 = f1_score(y_val, ensemble_val_pred)\n",
    "ensemble_proba = 1 / (1 + np.exp(-ensemble_val_avg))\n",
    "ensemble_auc = roc_auc_score(y_val, ensemble_proba)\n",
    "\n",
    "print(f\"\\nEnsemble One-Class SVM Results:\")\n",
    "print(f\"Precision: {ensemble_precision:.4f}\")\n",
    "print(f\"Recall: {ensemble_recall:.4f}\")\n",
    "print(f\"F1-Score: {ensemble_f1:.4f}\")\n",
    "print(f\"ROC-AUC: {ensemble_auc:.4f}\")\n",
    "\n",
    "# 2. Feature Selection for One-Class SVM\n",
    "print(\"\\n--- Feature Selection Analysis ---\")\n",
    "\n",
    "# Analyze feature variance in normal vs fraud transactions\n",
    "normal_transactions = X_train[y_train == 0]\n",
    "fraud_transactions = X_train[y_train == 1]\n",
    "\n",
    "feature_analysis = []\n",
    "for feature in X_train.columns[:20]:  # Analyze first 20 features\n",
    "    normal_mean = normal_transactions[feature].mean()\n",
    "    fraud_mean = fraud_transactions[feature].mean()\n",
    "    normal_std = normal_transactions[feature].std()\n",
    "    fraud_std = fraud_transactions[feature].std()\n",
    "    \n",
    "    # Calculate difference in means (normalized by combined std)\n",
    "    combined_std = np.sqrt((normal_std**2 + fraud_std**2) / 2)\n",
    "    if combined_std > 0:\n",
    "        effect_size = abs(normal_mean - fraud_mean) / combined_std\n",
    "    else:\n",
    "        effect_size = 0\n",
    "    \n",
    "    feature_analysis.append({\n",
    "        'Feature': feature,\n",
    "        'Normal_Mean': normal_mean,\n",
    "        'Fraud_Mean': fraud_mean,\n",
    "        'Effect_Size': effect_size\n",
    "    })\n",
    "\n",
    "feature_analysis_df = pd.DataFrame(feature_analysis).sort_values('Effect_Size', ascending=False)\n",
    "print(\"\\nTop 10 features with largest effect sizes (Normal vs Fraud):\")\n",
    "print(feature_analysis_df.head(10).round(4))\n",
    "\n",
    "# 3. Outlier Detection on Training Set\n",
    "print(\"\\n--- Outlier Analysis on Training Set ---\")\n",
    "\n",
    "train_predictions_final = final_oc_svm.predict(X_train_final_scaled)\n",
    "train_scores_final = final_oc_svm.decision_function(X_train_final_scaled)\n",
    "\n",
    "# Since we trained on normal transactions only, all predictions should be on validation/test\n",
    "# Let's analyze the training normal transactions\n",
    "train_normal_pred = (train_predictions_final == -1).astype(int)\n",
    "outlier_rate_in_normal = np.mean(train_normal_pred)\n",
    "\n",
    "print(f\"Outlier rate in 'normal' training data: {outlier_rate_in_normal:.4f}\")\n",
    "print(f\"Expected outlier rate (nu parameter): {best_params['nu']:.4f}\")\n",
    "\n",
    "if outlier_rate_in_normal > 0:\n",
    "    print(f\"Number of normal transactions flagged as outliers: {np.sum(train_normal_pred)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# BUSINESS IMPACT ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Assuming some business metrics (you can adjust these based on your domain)\n",
    "avg_transaction_value = 100  # Average transaction value in dollars\n",
    "fraud_investigation_cost = 50  # Cost to investigate each flagged transaction\n",
    "fraud_loss_prevented = 200  # Average loss prevented per caught fraud\n",
    "\n",
    "# Calculate business impact for validation set\n",
    "true_positives = np.sum((y_val == 1) & (ensemble_val_pred == 1))\n",
    "false_positives = np.sum((y_val == 0) & (ensemble_val_pred == 1))\n",
    "false_negatives = np.sum((y_val == 1) & (ensemble_val_pred == 0))\n",
    "true_negatives = np.sum((y_val == 0) & (ensemble_val_pred == 0))\n",
    "\n",
    "total_investigations = true_positives + false_positives\n",
    "investigation_cost = total_investigations * fraud_investigation_cost\n",
    "fraud_prevented_value = true_positives * fraud_loss_prevented\n",
    "fraud_missed_cost = false_negatives * fraud_loss_prevented\n",
    "\n",
    "net_benefit = fraud_prevented_value - investigation_cost - fraud_missed_cost\n",
    "\n",
    "print(f\"Business Impact Analysis (Validation Set):\")\n",
    "print(f\"True Positives (Fraud Caught): {true_positives}\")\n",
    "print(f\"False Positives (Normal Flagged): {false_positives}\")\n",
    "print(f\"False Negatives (Fraud Missed): {false_negatives}\")\n",
    "print(f\"True Negatives (Normal Passed): {true_negatives}\")\n",
    "print(f\"\\nCost Analysis:\")\n",
    "print(f\"Investigation Cost: ${investigation_cost:,.2f}\")\n",
    "print(f\"Fraud Loss Prevented: ${fraud_prevented_value:,.2f}\")\n",
    "print(f\"Fraud Loss from Missed Cases: ${fraud_missed_cost:,.2f}\")\n",
    "print(f\"Net Benefit: ${net_benefit:,.2f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL RECOMMENDATIONS AND SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RECOMMENDATIONS AND SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "ONE-CLASS SVM FRAUD DETECTION SUMMARY:\n",
    "\n",
    "Best Configuration:\n",
    "- Kernel: {best_kernel}\n",
    "- Scaler: {best_scaler_name}\n",
    "- Nu parameter: {best_params['nu']:.4f}\n",
    "- Optimal threshold: {optimal_threshold:.4f}\n",
    "\n",
    "Performance Metrics:\n",
    "- Precision: {final_precision:.4f} (of flagged transactions, {final_precision*100:.1f}% are actually fraud)\n",
    "- Recall: {final_recall:.4f} (catches {final_recall*100:.1f}% of all fraud cases)\n",
    "- F1-Score: {final_f1:.4f}\n",
    "- ROC-AUC: {final_auc:.4f}\n",
    "\n",
    "Strengths of One-Class SVM for Fraud Detection:\n",
    "✓ Excellent for highly imbalanced datasets\n",
    "✓ No need for fraud examples during training\n",
    "✓ Robust to noise and outliers\n",
    "✓ Works well with high-dimensional data\n",
    "✓ Can detect novel fraud patterns not seen in training\n",
    "\n",
    "Considerations:\n",
    "⚠ Computationally intensive for large datasets\n",
    "⚠ Sensitive to hyperparameter selection\n",
    "⚠ Less interpretable than tree-based models\n",
    "⚠ Performance depends on quality of 'normal' training data\n",
    "\n",
    "Recommendations:\n",
    "1. Use One-Class SVM as a complement to supervised methods\n",
    "2. Regularly retrain on recent normal transaction patterns\n",
    "3. Consider ensemble approaches for improved robustness\n",
    "4. Monitor performance over time due to concept drift\n",
    "5. Implement threshold monitoring and adjustment based on business feedback\n",
    "\n",
    "Business Value:\n",
    "- Net benefit of ${net_benefit:,.2f} on validation set\n",
    "- Efficient resource allocation for fraud investigations\n",
    "- Proactive fraud prevention capabilities\n",
    "\"\"\")\n",
    "\n",
    "# Optional: Save models and results\n",
    "print(f\"\\nModels and predictions ready for deployment!\")\n",
    "print(f\"- Best One-Class SVM model: final_oc_svm\")\n",
    "print(f\"- Ensemble predictions available: ensemble_test_pred\")\n",
    "print(f\"- Submission files: submission_default, submission_optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPORT FUNCTIONS FOR PRODUCTION USE\n",
    "# =============================================================================\n",
    "\n",
    "def predict_fraud_ocsvm(new_transactions, model=final_oc_svm, scaler=final_scaler, threshold=optimal_threshold):\n",
    "    \"\"\"\n",
    "    Function to predict fraud on new transactions using trained One-Class SVM\n",
    "    \n",
    "    Parameters:\n",
    "    - new_transactions: DataFrame with same features as training data\n",
    "    - model: Trained One-Class SVM model\n",
    "    - scaler: Fitted scaler object\n",
    "    - threshold: Decision threshold for fraud classification\n",
    "    \n",
    "    Returns:\n",
    "    - predictions: Binary predictions (1 for fraud, 0 for normal)\n",
    "    - scores: Anomaly scores (lower = more anomalous)\n",
    "    - probabilities: Fraud probabilities\n",
    "    \"\"\"\n",
    "    # Remove ID columns if present\n",
    "    features = new_transactions.drop(['TransactionID', 'uid'], axis=1, errors='ignore')\n",
    "    \n",
    "    # Scale features\n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    # Get predictions and scores\n",
    "    raw_predictions = model.predict(features_scaled)\n",
    "    scores = model.decision_function(features_scaled)\n",
    "    \n",
    "    # Apply threshold\n",
    "    binary_predictions = (scores < threshold).astype(int)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probabilities = 1 / (1 + np.exp(-scores))\n",
    "    \n",
    "    return binary_predictions, scores, probabilities\n",
    "\n",
    "def get_fraud_confidence(scores, thresholds_dict):\n",
    "    \"\"\"\n",
    "    Categorize fraud confidence based on decision function scores\n",
    "    \"\"\"\n",
    "    conditions = [\n",
    "        scores < thresholds_dict['high_confidence'],\n",
    "        scores < thresholds_dict['medium_confidence'],\n",
    "        scores < thresholds_dict['low_confidence']\n",
    "    ]\n",
    "    choices = ['High Confidence Fraud', 'Medium Confidence Fraud', 'Low Confidence Fraud']\n",
    "    \n",
    "    return np.select(conditions, choices, default='Normal Transaction')\n",
    "\n",
    "# Define confidence thresholds based on percentiles\n",
    "confidence_thresholds = {\n",
    "    'high_confidence': np.percentile(final_val_scores, 1),\n",
    "    'medium_confidence': np.percentile(final_val_scores, 5),\n",
    "    'low_confidence': np.percentile(final_val_scores, 10)\n",
    "}\n",
    "\n",
    "# Apply to test set\n",
    "test_confidence = get_fraud_confidence(test_scores, confidence_thresholds)\n",
    "confidence_distribution = pd.Series(test_confidence).value_counts()\n",
    "\n",
    "print(f\"\\nTest Set Fraud Confidence Distribution:\")\n",
    "print(confidence_distribution)\n",
    "\n",
    "# Create detailed submission with confidence levels\n",
    "submission_detailed = pd.DataFrame({\n",
    "    'TransactionID': testX['TransactionID'],\n",
    "    'isFraud': test_pred_optimized,\n",
    "    'fraud_score': test_scores,\n",
    "    'fraud_probability': 1 / (1 + np.exp(-test_scores)),\n",
    "    'confidence_level': test_confidence\n",
    "})\n",
    "\n",
    "print(f\"\\nDetailed submission sample (first 10 high-risk transactions):\")\n",
    "high_risk_sample = submission_detailed[submission_detailed['isFraud'] == 1].head(10)\n",
    "print(high_risk_sample[['TransactionID', 'fraud_probability', 'confidence_level']].round(4))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ONE-CLASS SVM ANALYSIS COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total fraudulent transactions detected: {np.sum(test_pred_optimized)}\")\n",
    "print(f\"Detection rate: {np.mean(test_pred_optimized)*100:.2f}%\")\n",
    "print(f\"Ready for production deployment!\")\n",
    "\n",
    "# Optional: Uncomment to save results\n",
    "# submission_default.to_csv('one_class_svm_predictions_default.csv', index=False)\n",
    "# submission_optimized.to_csv('one_class_svm_predictions_optimized.csv', index=False)\n",
    "# submission_detailed.to_csv('one_class_svm_predictions_detailed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieee-cis-fraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
